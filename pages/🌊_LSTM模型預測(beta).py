import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.io as pio
from plotly.subplots import make_subplots 
import os
import io
import json
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr 
import zipfile

# --- æ–°å¢ ---
import joblib
import hashlib

from utils.helpers import get_station_name_from_id, initialize_session_state, load_data

pio.templates.default = "plotly_white"

# --- å˜—è©¦å°å…¥ TensorFlow / Keras ---
tensorflow_available = False
try:
    import tensorflow as tf
    from keras.models import Sequential, Model
    from keras.layers import LSTM, Dense, Dropout, Input 
    from keras.callbacks import EarlyStopping, Callback 
    from keras.models import load_model # --- æ–°å¢ ---
    tensorflow_available = True
except ImportError:
    st.error("éŒ¯èª¤ï¼šTensorFlow/Keras åº«æœªå®‰è£æˆ–ç„¡æ³•è¼‰å…¥ã€‚LSTM æ¨¡å‹é æ¸¬åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")
    st.info("è‹¥éœ€ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œè«‹åœ¨æ‚¨çš„ Python ç’°å¢ƒä¸­é‹è¡Œä»¥ä¸‹å‘½ä»¤ï¼š")
    st.code("pip install tensorflow scikit-learn numpy plotly scipy joblib")
    st.warning("ç¢ºä¿æ‚¨çš„ TensorFlow å®‰è£èˆ‡æ‚¨çš„ç³»çµ±å’Œ CUDA ç‰ˆæœ¬å…¼å®¹ (å¦‚æœä½¿ç”¨ GPU)ã€‚")

# --- æ–°å¢ï¼šæ¨¡å‹å¿«å–è¼”åŠ©å‡½å¼ ---
def analyze_data_quality(df, relevant_params):
    report = {}
    for param_col in relevant_params:
        if param_col not in df.columns:
            continue
        
        s = df[param_col]
        total_records = len(s)
        missing_count = s.isnull().sum()
        valid_count = total_records - missing_count
        
        is_numeric = pd.api.types.is_numeric_dtype(s)
        param_metrics = {
            'total_records': total_records, 'valid_count': valid_count, 'missing_count': missing_count,
            'missing_percentage': (missing_count / total_records * 100) if total_records > 0 else 0,
            'is_numeric': is_numeric
        }

        if is_numeric and valid_count > 0:
            param_metrics['zero_count'] = (s == 0).sum()
            param_metrics['negative_count'] = (s < 0).sum()
            Q1 = s.quantile(0.25)
            Q3 = s.quantile(0.75)
            IQR = Q3 - Q1
            if IQR > 0:
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                param_metrics['outlier_iqr_count'] = ((s < lower_bound) | (s > upper_bound)).sum()
            else:
                param_metrics['outlier_iqr_count'] = 0
        else:
             param_metrics['zero_count'] = 0
             param_metrics['negative_count'] = 0
             param_metrics['outlier_iqr_count'] = 0

        report[param_col] = param_metrics
    return report

def assess_risk(value, param_key):
    """æ ¹æ“šé æ¸¬å€¼ã€åƒæ•¸åç¨±å’Œè¨­å®šæª”ï¼Œå›å‚³é¢¨éšªç­‰ç´š"""
    # å¾ config ä¸­è®€å– risk_thresholds å€å¡Šï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡å›å‚³ç©ºå­—å…¸
    thresholds = st.session_state['risk_thresholds'].get(param_key, {})
    
    # å¦‚æœ config ä¸­æ²’æœ‰è¨­å®šæ­¤åƒæ•¸çš„é–¾å€¼ï¼Œå‰‡å›å‚³"æœªçŸ¥"
    if not thresholds:
        return "æœªçŸ¥"
    
    # è®€å–å±éšªå’Œè­¦å‘Šç­‰ç´šï¼Œå¦‚æœæ²’æœ‰è¨­å®šï¼Œå‰‡è¨­ç‚ºç„¡é™å¤§
    danger_level = thresholds.get("danger", float('inf'))
    warning_level = thresholds.get("warning", float('inf'))

    if value >= danger_level:
        return "å±éšª"
    elif value >= warning_level:
        return "è­¦å‘Š"
    else:
        return "å®‰å…¨"
def get_local_model_paths(parameters: dict):
    """æ ¹æ“šåƒæ•¸å­—å…¸ç”Ÿæˆå”¯ä¸€çš„æ¨¡å‹ã€scalerå’Œhistoryè·¯å¾‘"""
    model_dir = "trained_models"
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
        
    config_str = "".join([f"{k}:{v}" for k, v in sorted(parameters.items())])
    model_hash = hashlib.md5(config_str.encode()).hexdigest()
    
    model_path = os.path.join(model_dir, f"lstm_model_{model_hash}.keras")
    scaler_path = os.path.join(model_dir, f"lstm_scaler_{model_hash}.joblib")
    # æ–°å¢ history è·¯å¾‘
    history_path = os.path.join(model_dir, f"lstm_history_{model_hash}.json")
    
    return model_path, scaler_path, history_path

def save_local_model(model, scaler, history_data: dict, parameters: dict):
    """ä¿å­˜æ¨¡å‹ã€scaler å’Œè¨“ç·´æ­·å²"""
    try:
        model_path, scaler_path, history_path = get_local_model_paths(parameters)
        model.save(model_path)
        joblib.dump(scaler, scaler_path)
        # å°‡ history å­—å…¸å­˜æˆ json
        with open(history_path, 'w') as f:
            json.dump(history_data, f)
        st.toast(f"æ¨¡å‹èˆ‡è¨“ç·´æ­·å²å·²æˆåŠŸå¿«å–ï¼")
    except Exception as e:
        st.warning(f"å„²å­˜æ¨¡å‹å¿«å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def load_local_model(parameters: dict):
    """å˜—è©¦è¼‰å…¥å·²ä¿å­˜çš„æ¨¡å‹ã€scaler å’Œè¨“ç·´æ­·å²"""
    model_path, scaler_path, history_path = get_local_model_paths(parameters)
    
    # ç¢ºèªä¸‰å€‹æª”æ¡ˆéƒ½å­˜åœ¨
    if os.path.exists(model_path) and os.path.exists(scaler_path) and os.path.exists(history_path):
        try:
            tf.keras.backend.clear_session()
            model = load_model(model_path)
            scaler = joblib.load(scaler_path)
            # è®€å– history json
            with open(history_path, 'r') as f:
                history_data = json.load(f)
            return model, scaler, history_data
        except Exception as e:
            st.warning(f"è¼‰å…¥å¿«å–æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚å°‡é‡æ–°é€²è¡Œè¨“ç·´ã€‚")
            return None, None, None
            
    return None, None, None

# --- æ–°å¢ï¼šå°‡ AccuracyHistory é¡åˆ¥çš„å®šç¾©ç§»åˆ°é ‚å±¤ ---

class StreamlitProgressBar(Callback):
    """ä¸€å€‹ç”¨æ–¼åœ¨ Streamlit ä¸­é¡¯ç¤º Keras è¨“ç·´é€²åº¦çš„ Callbackã€‚"""
    def __init__(self, epochs):
        super().__init__()
        self.epochs = epochs
        # åœ¨ Streamlit ä»‹é¢ä¸Šå»ºç«‹ä¸€å€‹é€²åº¦æ¢å…ƒä»¶å’Œä¸€å€‹ç©ºçš„æ–‡å­—ä½ç½®
        self.progress_bar = st.progress(0)
        self.status_text = st.empty()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        # è¨ˆç®—ç›®å‰é€²åº¦
        progress = (epoch + 1) / self.epochs
        
        # æ›´æ–°é€²åº¦æ¢çš„é•·åº¦
        self.progress_bar.progress(progress)
        
        # å¾ logs å­—å…¸ä¸­ç²å–æœ€æ–°çš„ loss å€¼ä¸¦é¡¯ç¤º
        loss = logs.get('loss', 0)
        val_loss = logs.get('val_loss', 0)
        status = f"Epoch {epoch + 1}/{self.epochs} - Loss: {loss:.4f}, Val Loss: {val_loss:.4f}"
        
        # æ›´æ–°ç‹€æ…‹æ–‡å­—
        self.status_text.text(status)

    def on_train_end(self, logs=None):
        # è¨“ç·´çµæŸå¾Œï¼Œæ¸…é™¤é€²åº¦æ¢å’Œæ–‡å­—ï¼Œä¿æŒä»‹é¢ä¹¾æ·¨
        self.progress_bar.empty()
        self.status_text.empty()
class AccuracyHistory(Callback):
    def __init__(self, X_train, y_train, X_test, y_test, scaler, epsilon, look_back):
        super().__init__()
        self.X_train, self.y_train = X_train, y_train
        self.X_test, self.y_test = X_test, y_test
        self.scaler, self.epsilon = scaler, epsilon
        # é€™äº›åˆ—è¡¨ç¾åœ¨ä¸»è¦ç”¨æ–¼ç¹ªè£½å³æ™‚çš„è¨“ç·´æ›²ç·š
        self.train_accuracies, self.val_accuracies = [], []
        self.train_correlations, self.val_correlations = [], []

    # 1. å°‡æ‰€æœ‰è¨ˆç®—é‚è¼¯ç§»åˆ°ä¸€å€‹æ–°å‡½å¼ä¸­ï¼Œä¸¦æ¥æ”¶ model ä½œç‚ºåƒæ•¸
    def calculate_metrics(self, model):
        """æ‰‹å‹•è¨ˆç®—ä¸¦è¨˜éŒ„ä¸€æ¬¡æº–ç¢ºç‡å’Œç›¸é—œä¿‚æ•¸"""
        train_pred_scaled = model.predict(self.X_train, verbose=0)
        train_actual_scaled = self.y_train.reshape(-1, 1)
        train_pred_original = self.scaler.inverse_transform(train_pred_scaled)
        train_actual_original = self.scaler.inverse_transform(train_actual_scaled)
        train_correct_count = np.sum(np.abs(train_pred_original - train_actual_original) <= self.epsilon)
        self.train_accuracies.append(train_correct_count / len(train_actual_original) if len(train_actual_original) > 0 else 0)
        if len(train_actual_original) > 1 and len(train_pred_original.flatten()) > 1:
            train_corr, _ = pearsonr(train_actual_original.flatten(), train_pred_original.flatten())
            self.train_correlations.append(train_corr)
        else: self.train_correlations.append(np.nan)

        val_pred_scaled = model.predict(self.X_test, verbose=0)
        val_actual_scaled = self.y_test.reshape(-1, 1)
        val_pred_original = self.scaler.inverse_transform(val_pred_scaled)
        val_actual_original = self.scaler.inverse_transform(val_actual_scaled)
        val_correct_count = np.sum(np.abs(val_pred_original - val_actual_original) <= self.epsilon)
        self.val_accuracies.append(val_correct_count / len(val_actual_original) if len(val_actual_original) > 0 else 0)
        if len(val_actual_original) > 1 and len(val_pred_original.flatten()) > 1:
            val_corr, _ = pearsonr(val_actual_original.flatten(), val_pred_original.flatten())
            self.val_correlations.append(val_corr)
        else: self.val_correlations.append(np.nan)

    # 2. è®“ on_epoch_end åœ¨è¨“ç·´æ™‚å‘¼å«é€™å€‹æ–°å‡½å¼
    def on_epoch_end(self, epoch, logs=None):
        """æ­¤å‡½å¼ç”± Keras åœ¨æ¯å€‹ epoch çµæŸæ™‚è‡ªå‹•å‘¼å«"""
        if logs is None:
            logs = {}
        
        # åŸ·è¡Œè¨ˆç®—
        self.calculate_metrics(self.model)
        
        # <<< æ ¸å¿ƒä¿®æ”¹ï¼šå°‡æœ€æ–°çš„è¨ˆç®—çµæœå›å¯«åˆ° logs ä¸­ >>>
        # é€™æ¨£ Keras å°±æœƒè‡ªå‹•æŠŠé€™äº›æŒ‡æ¨™è¨˜éŒ„åˆ° history.history ç‰©ä»¶è£¡
        logs['train_accuracy'] = self.train_accuracies[-1]
        logs['val_accuracy'] = self.val_accuracies[-1]
        logs['train_correlation'] = self.train_correlations[-1]
        logs['val_correlation'] = self.val_correlations[-1]

# # TODO: Reset chat history
def chat_system():
    print(1)
    st.header("ğŸ¤– AI å•ç­”")
    if 'chat_history' not in st.session_state:
        st.session_state['chat_history'] = []
    pipeline = load_chat_pipeline()

    col1, col2 = st.columns([4, 1])
    with col1:
        user_input = st.text_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ:", key='lstm_chat_input', label_visibility='collapsed')
    with col2:
        if st.button("ç™¼é€", key='lstm_chat_send') and user_input.strip():
                st.session_state['chat_history'].append(("user", user_input.strip()))
                response = pipeline(
                        f"""
<æ•¸æ“šé›†åƒæ•¸>
{json.dumps(st.session_state.get('parameter_info', {}), ensure_ascii=False)}
</æ•¸æ“šé›†åƒæ•¸>
<æ¨¡å‹åƒæ•¸>
{json.dumps(st.session_state.get('risk_thresholds', {}), ensure_ascii=False)}
</æ¨¡å‹åƒæ•¸>
<è¨“ç·´æ•¸æ“š>
å°šæœªå¯¦ä½œ
</è¨“ç·´æ•¸æ“š>
<æ•¸æ“šå“è³ª>
å°šæœªå¯¦ä½œ
</æ•¸æ“šå“è³ª>
<æ¨¡å‹æ€§èƒ½>
å°šæœªå¯¦ä½œ
</æ¨¡å‹æ€§èƒ½>
<é æ¸¬çµæœ>
å°šæœªå¯¦ä½œ
</é æ¸¬çµæœ>
<å°ˆæœ‰åè©>
å°šæœªå¯«å…¥
</å°ˆæœ‰åè©>
<è¼¸å‡ºæ ¼å¼>
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡é¡¯ç¤º
</è¼¸å‡ºæ ¼å¼>
<ç”¨æˆ¶å•é¡Œ>
{user_input.strip()}
</ç”¨æˆ¶å•é¡Œ>
                        """.strip()

                )
                st.session_state['chat_history'].append(("bot", response))

    if st.session_state['chat_history']:
        with st.expander("æŸ¥çœ‹å°è©±æ­·å²", expanded=True):
            for role, msg in st.session_state['chat_history']:
                if role == "user":
                    st.markdown(f"**æ‚¨:** {msg}")
                else:
                    st.markdown(f"**AI:** {msg}")

async def call_kuwa_api(x):
    token = os.getenv('KUWA_TOKEN')
    if not token:
        st.warning("ç¼ºå°‘ Kuwa Token")

    client = KuwaClient(
        base_url="http://127.0.0.1",
        model=".bot/Llama 3.2 3B @NPU",
        auth_token=token,
    )
    generator = client.chat_complete(messages=[{"role": "user", "content": x}], streaming=False)

    result = ""
    async for chunk in generator:
        result += chunk
    return result

@st.cache_resource
def load_chat_pipeline():    
   

    def generator(x):
        return asyncio.run(call_kuwa_api(x))

    return generator

# --- è¨­å®šé é¢ ---
st.set_page_config(
    page_title="LSTM æ¨¡å‹é æ¸¬",
    page_icon="ğŸŒŠ",
    layout="wide"
)
initialize_session_state()

st.title("ğŸŒŠ æµ·æ´‹æ•¸æ“š LSTM æ¨¡å‹é æ¸¬")
st.markdown("ä½¿ç”¨é•·çŸ­æœŸè¨˜æ†¶ (LSTM) é¡ç¥ç¶“ç¶²çµ¡é æ¸¬æµ·æ´‹æ•¸æ“šçš„æœªä¾†è¶¨å‹¢ã€‚")

predictable_params_config_map = {
    col_name: info["display_zh"] for col_name, info in st.session_state.get('parameter_info', {}).items()
    if info.get("type") == "linear"
}

def create_sequences(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i + look_back), 0])
        y.append(data[i + look_back, 0])
    return np.array(X), np.array(y)

# --- å´é‚Šæ¬„ï¼šLSTM é æ¸¬è¨­å®šæ§åˆ¶é … ---
st.sidebar.header("LSTM é æ¸¬è¨­å®š")

locations = st.session_state.get('locations', [])

if not locations:
    st.sidebar.warning("è«‹åœ¨ `config.json` çš„ `STATION_COORDS` ä¸­é…ç½®æ¸¬ç«™è³‡è¨Šã€‚")
    st.stop()

selected_station = st.sidebar.selectbox("é¸æ“‡æ¸¬ç«™:", locations, key='pages_10_lstm_station', format_func=get_station_name_from_id)
selected_station_name = get_station_name_from_id(selected_station)
df_initial_check = load_data(selected_station, st.session_state.get('parameter_info', {}))

available_predictable_params_display_to_col = {}
if not df_initial_check.empty:
    for col_name_config, display_name in predictable_params_config_map.items():
        param_info_for_check = st.session_state['parameter_info'].get(col_name_config, {})
        expected_names = [col_name_config.lower()]
        if "column_name_in_data" in param_info_for_check: expected_names.append(param_info_for_check["column_name_in_data"].lower())
        if "display_zh" in param_info_for_check: expected_names.append(param_info_for_check["display_zh"].lower())
        
        target_col_to_check = next((name for name in expected_names if name in df_initial_check.columns), None)
        
        if target_col_to_check and pd.api.types.is_numeric_dtype(df_initial_check[target_col_to_check]) and df_initial_check[target_col_to_check].count() > 0:
            available_predictable_params_display_to_col[display_name] = target_col_to_check

if not available_predictable_params_display_to_col:
    st.sidebar.error("è¼‰å…¥æ•¸æ“šå¾Œï¼Œæ²’æœ‰å¯ä¾›é æ¸¬çš„æœ‰æ•ˆæ•¸å€¼å‹åƒæ•¸ã€‚")
    st.stop()

selected_param_display = st.sidebar.selectbox("é¸æ“‡é æ¸¬åƒæ•¸:", list(available_predictable_params_display_to_col.keys()), key='pages_10_lstm_param_display', format_func=lambda x: x if x in available_predictable_params_display_to_col else "æœªçŸ¥åƒæ•¸")
selected_param_col = available_predictable_params_display_to_col[selected_param_display]

param_unit = next((info.get("unit", "") for key, info in st.session_state.get('parameter_info', {}).items() if key == selected_param_col), "")

st.sidebar.markdown("---")
st.sidebar.subheader("é æ¸¬æ™‚é–“è¨­å®š")
prediction_frequencies = {"å°æ™‚ (H)": "h", "å¤© (D)": "D", "é€± (W)": "W", "æœˆ (M)": "ME", "å¹´ (Y)": "YE"}
selected_prediction_freq_display = st.sidebar.selectbox("é¸æ“‡é æ¸¬é »æ¬¡:", list(prediction_frequencies.keys()), key='pages_10_prediction_frequency')
selected_freq_pandas = prediction_frequencies[selected_prediction_freq_display]
forecast_period_value = st.sidebar.number_input(f"é æ¸¬æœªä¾†å¤šä¹… ({selected_prediction_freq_display.split(' ')[0]}):",min_value=1, value=24, step=1, key='pages_10_forecast_period_value')
st.sidebar.markdown("---")
st.sidebar.subheader("è¨“ç·´æ•¸æ“šæ™‚é–“ç¯„åœ")

if not df_initial_check.empty and 'ds' in df_initial_check.columns and not df_initial_check['ds'].isnull().all():
    min_date_available = df_initial_check['ds'].min().date()
    max_date_available = df_initial_check['ds'].max().date()
else:
    min_date_available = pd.to_datetime('1990-01-01').date()
    max_date_available = pd.Timestamp.now().date()

train_start_date = st.sidebar.date_input("è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸ:", value=min_date_available, min_value=min_date_available, max_value=max_date_available, key='pages_10_train_start_date')
train_end_date = st.sidebar.date_input("è¨“ç·´æ•¸æ“šçµæŸæ—¥æœŸ:", value=max_date_available, min_value=min_date_available, max_value=max_date_available, key='pages_10_train_end_date')

if train_start_date >= train_end_date:
    st.sidebar.error("è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸå¿…é ˆæ—©æ–¼çµæŸæ—¥æœŸã€‚")
    st.stop()

st.sidebar.markdown("---")
st.sidebar.subheader("æ•¸æ“šé è™•ç†")
missing_value_strategy = st.sidebar.selectbox("ç¼ºå¤±å€¼è™•ç†:", options=['å‰å‘å¡«å…… (ffill)', 'å¾Œå‘å¡«å…… (bfill)', 'ç·šæ€§æ’å€¼ (interpolate)', 'ç§»é™¤ç¼ºå¤±å€¼ (dropna)'], key='pages_10_missing_strategy')
apply_smoothing = st.sidebar.checkbox("æ‡‰ç”¨æ•¸æ“šå¹³æ»‘", value=False, key='pages_10_apply_smoothing')
smoothing_window = 1
if apply_smoothing:
    smoothing_window = st.sidebar.slider("å¹³æ»‘è™•ç† (ç§»å‹•å¹³å‡è¦–çª—):", min_value=1, max_value=24, value=3, step=1)
st.sidebar.markdown("---")
st.sidebar.subheader("æ¨¡å‹æº–ç¢ºç‡è¨­å®š")
epsilon_value = st.sidebar.number_input("æº–ç¢ºç‡ Îµ èª¤å·®å€é–“:", min_value=0.001, max_value=10.0, value=0.1, step=0.01, format="%.3f", help="è¨­å®šä¸€å€‹èª¤å·®ç¯„åœ Îµã€‚ç•¶ |é æ¸¬å€¼ - å¯¦éš›å€¼| <= Îµ æ™‚ï¼Œæ­¤é æ¸¬è¢«è¦–ç‚ºã€Œæ­£ç¢ºã€ã€‚", key='pages_10_epsilon_value')
st.sidebar.markdown("---")
st.sidebar.subheader("LSTM æ¨¡å‹åƒæ•¸")
look_back = st.sidebar.slider("å›æº¯æ™‚é–“æ­¥ (look_back):", 1, 48, 6, 1)
lstm_units = st.sidebar.slider("LSTM å±¤å–®å…ƒæ•¸:", 10, 200, 50, 10)
epochs = st.sidebar.number_input("è¨“ç·´è¿­ä»£æ¬¡æ•¸ (Epochs):", 10, 500, 50, 10)
batch_size = st.sidebar.number_input("æ‰¹æ¬¡å¤§å° (Batch Size):", 1, 128, 32, 8)
dropout_rate = st.sidebar.slider("Dropout æ¯”ç‡:", 0.0, 0.5, 0.2, 0.05)
validation_split = st.sidebar.slider("é©—è­‰é›†æ¯”ä¾‹:", 0.0, 0.5, 0.1, 0.05)
patience = st.sidebar.number_input("æ—©åœè€å¿ƒå€¼ (Patience):", min_value=5, max_value=200, value=50, step=5)

if st.sidebar.button("ğŸŒŠ åŸ·è¡Œ LSTM é æ¸¬") or st.session_state.get('success', False):
    if not tensorflow_available:
        st.error("TensorFlow/Keras åº«ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œ LSTM é æ¸¬ã€‚")
        st.stop()

    if df_initial_check.empty or selected_param_col not in df_initial_check.columns:
        st.error(f"æ‰€é¸æ¸¬ç«™ '{selected_station_name}' çš„æ•¸æ“šæ–‡ä»¶ç¼ºå°‘åƒæ•¸ '{selected_param_display}'ã€‚")
        st.stop()
    
    with st.spinner("STEP 1/3: æ­£åœ¨é è™•ç†æ•¸æ“š..."):
        df_processed = df_initial_check[['ds', selected_param_col]].copy()
        df_processed.columns = ['ds', 'y']
        
        train_start_datetime = pd.to_datetime(train_start_date)
        train_end_datetime = pd.to_datetime(train_end_date) + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)
        df_processed = df_processed[(df_processed['ds'] >= train_start_datetime) & (df_processed['ds'] <= train_end_datetime)].copy()
        
        if df_processed.empty:
            st.error(f"åœ¨é¸å®šçš„è¨“ç·´æ™‚é–“ç¯„åœ ({train_start_date} è‡³ {train_end_date}) å…§æ²’æœ‰æ‰¾åˆ°æ•¸æ“šã€‚")
            st.stop()
            
        df_processed = df_processed.set_index('ds').resample(selected_freq_pandas).mean().reset_index()
        
        if missing_value_strategy == 'å‰å‘å¡«å…… (ffill)': df_processed['y'] = df_processed['y'].ffill()
        elif missing_value_strategy == 'å¾Œå‘å¡«å…… (bfill)': df_processed['y'] = df_processed['y'].bfill()
        elif missing_value_strategy == 'ç·šæ€§æ’å€¼ (interpolate)': df_processed['y'] = df_processed['y'].interpolate(method='linear')
        
        if apply_smoothing and smoothing_window > 1:
            df_processed['y'] = df_processed['y'].rolling(window=smoothing_window, min_periods=1, center=True).mean()
            
        df_processed.dropna(subset=['ds', 'y'], inplace=True)
        
        if df_processed.empty or len(df_processed) <= look_back:
            st.error(f"ç¶“éæ•¸æ“šé è™•ç†å¾Œï¼Œæ²’æœ‰è¶³å¤ çš„æœ‰æ•ˆæ•¸æ“šç”¨æ–¼é æ¸¬ (é•·åº¦éœ€å¤§æ–¼ {look_back})ã€‚")
            st.stop()

    model_params = {
        "page": "lstm_prediction", "station": selected_station_name, "param": selected_param_col,
        "freq": selected_freq_pandas, "look_back": look_back, "lstm_units": lstm_units, "epochs": epochs,
        "batch_size": batch_size, "dropout": dropout_rate, "smoothing": smoothing_window if apply_smoothing else 0,
        "start_date": train_start_date.strftime('%Y-%m-%d'), "end_date": train_end_date.strftime('%Y-%m-%d'),
        "missing_strategy": missing_value_strategy
    }

    model, scaler, history_data = load_local_model(model_params)
    history = None 

    if model is None:
        st.info("ğŸ› ï¸ æœªæ‰¾åˆ°å¿«å–æ¨¡å‹ï¼Œé–‹å§‹æ–°çš„è¨“ç·´...")
        st.write("---") 
        st.write("**STEP 2/3: æ­£åœ¨è¨“ç·´ LSTM æ¨¡å‹...**") # ç”¨ä¸€èˆ¬æ–‡å­—æ¨™é¡Œå–ä»£ spinner
        
        # æ•¸æ“šç¸®æ”¾èˆ‡å¡‘å½¢ (é€™éƒ¨åˆ†ç¨‹å¼ç¢¼ä¸è®Š)
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(df_processed['y'].values.reshape(-1, 1))
        X, y = create_sequences(scaled_data, look_back)
        train_size = int(len(X) * (1 - validation_split))
        X_train, X_test = X[0:train_size,:], X[train_size:len(X),:]
        y_train, y_test = y[0:train_size], y[train_size:len(y)]
        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
        
        # å»ºç«‹æ¨¡å‹ (é€™éƒ¨åˆ†ç¨‹å¼ç¢¼ä¸è®Š)
        accuracy_history_callback = AccuracyHistory(X_train, y_train, X_test, y_test, scaler, epsilon_value, look_back)
        model = Sequential([
            Input(shape=(look_back, 1)),
            LSTM(lstm_units, return_sequences=True), Dropout(dropout_rate),
            LSTM(lstm_units, return_sequences=False), Dropout(dropout_rate),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mean_squared_error')
        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)
        
        # <<< æ ¸å¿ƒä¿®æ”¹é» >>>
        # 1. åœ¨é€™è£¡å»ºç«‹æˆ‘å€‘çš„é€²åº¦æ¢ç‰©ä»¶
        progress_callback = StreamlitProgressBar(epochs)
        
        try:
            # 2. å°‡ progress_callback åŠ å…¥ callbacks åˆ—è¡¨
            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, 
                                validation_data=(X_test, y_test), 
                                callbacks=[early_stopping, accuracy_history_callback, progress_callback], 
                                verbose=0)
            history_data = history.history
        except Exception as e:
            st.error(f"LSTM æ¨¡å‹è¨“ç·´å¤±æ•—ï¼š{e}")
            st.stop()
            
        st.success("æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        save_local_model(model, scaler, history_data, model_params)
    else:
        st.success("âœ… æˆåŠŸè¼‰å…¥å¿«å–æ¨¡å‹ï¼")
        with st.spinner("STEP 2/3: æ­£åœ¨æº–å‚™æ•¸æ“š..."):
            scaled_data = scaler.transform(df_processed['y'].values.reshape(-1, 1))
            X, y = create_sequences(scaled_data, look_back)
            train_size = int(len(X) * (1 - validation_split))
            X_train, X_test = X[0:train_size,:], X[train_size:len(X),:]
            y_train, y_test = y[0:train_size], y[train_size:len(y)]
            X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
            accuracy_history_callback = AccuracyHistory(X_train, y_train, X_test, y_test, scaler, epsilon_value, look_back)

        st.session_state['success'] = True

    with st.spinner("STEP 3/3: æ­£åœ¨è©•ä¼°èˆ‡è¦–è¦ºåŒ–..."):

        chat_system()

        st.subheader("ğŸ“š è¨“ç·´æ•¸æ“šæ¦‚è¦½")
        if not df_processed.empty:
            total_duration = df_processed['ds'].max() - df_processed['ds'].min()
            total_records = len(df_processed)
            inferred_freq = None
            try:
                # ä½¿ç”¨ .copy() é¿å…æ½›åœ¨çš„ SettingWithCopyWarning
                inferred_freq = pd.infer_freq(df_processed['ds'].copy())
            except ValueError:
                inferred_freq = 'ç„¡æ³•ç²¾ç¢ºæ¨æ–· (æ•¸æ“šå¯èƒ½é–“éš”ä¸ä¸€è‡´)'
            
            st.write(f"**ä½¿ç”¨æ•¸æ“šå€é–“**: å¾ **{df_processed['ds'].min().strftime('%Y-%m-%d %H:%M')}** åˆ° **{df_processed['ds'].max().strftime('%Y-%m-%d %H:%M')}**")
            st.write(f"**ç¸½æ™‚é•·**: **{total_duration}**")
            st.write(f"**ç¸½ç­†æ•¸**: **{total_records}** ç­†")
            st.write(f"**æ•¸æ“šé »æ¬¡ (é è™•ç†å¾Œ)**: **{selected_freq_pandas}** (åŸå§‹æ¨æ–·: **{inferred_freq or 'N/A'}**)")
        else:
            st.warning("æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ•¸æ“šæ¦‚è¦½ã€‚")

        st.subheader("ğŸ“Š æ•¸æ“šå“è³ªæ¦‚è¦½")
        # æˆ‘å€‘è¦åˆ†æçš„æ˜¯ç¶“éæ™‚é–“ç¯„åœç¯©é¸å’Œé è™•ç†å¾Œçš„ df_processed
        df_for_quality_check = df_processed.rename(columns={'y': selected_param_col})
        quality_report = analyze_data_quality(df_for_quality_check, [selected_param_col])

        if selected_param_col in quality_report:
            metrics = quality_report[selected_param_col]
            
            # ä½¿ç”¨ st.columns è®“æ’ç‰ˆæ›´å¥½çœ‹
            c1, c2 = st.columns([1, 2])
            with c1:
                st.write(f"**åˆ†æåƒæ•¸: {selected_param_display}**")
                st.metric("ç¸½ç­†æ•¸", metrics['total_records'])
                st.metric("æœ‰æ•ˆç­†æ•¸", metrics['valid_count'])
                st.metric("ç¼ºå¤±æ¯”ä¾‹", f"{metrics['missing_percentage']:.2f}%")
                st.metric("æ½›åœ¨ç•°å¸¸å€¼", metrics['outlier_iqr_count'])
            
            with c2:
                quality_data = {
                    'é¡å‹': ['æœ‰æ•ˆå€¼', 'ç¼ºå¤±å€¼', 'é›¶å€¼', 'è² å€¼', 'æ½›åœ¨ç•°å¸¸å€¼'], 
                    'æ•¸é‡': [
                        metrics['valid_count'], metrics['missing_count'], 
                        metrics['zero_count'], metrics['negative_count'], 
                        metrics['outlier_iqr_count']
                    ]
                }
                quality_df = pd.DataFrame(quality_data)
                quality_df = quality_df[quality_df['æ•¸é‡'] > 0]

                if not quality_df.empty:
                    # ä¿®æ­£ï¼šä½¿ç”¨ selected_param_display
                    fig_quality = px.pie(quality_df, values='æ•¸é‡', names='é¡å‹', 
                                         title=f"'{selected_param_display}' æ•¸æ“šå“è³ªåˆ†ä½ˆ", 
                                         hole=0.3, color_discrete_sequence=px.colors.qualitative.Pastel)
                    fig_quality.update_traces(textposition='inside', textinfo='percent+label')
                    st.plotly_chart(fig_quality, use_container_width=True, key="quality_chart")
                else:
                    st.info("æ•¸æ“šå“è³ªéå¸¸é«˜ï¼Œæ²’æœ‰åµæ¸¬åˆ°ç¼ºå¤±ã€é›¶ã€è² å€¼æˆ–ç•°å¸¸å€¼ã€‚")
        st.write("---")
        st.subheader("ğŸ“‰ æ¨¡å‹æ€§èƒ½è©•ä¼°")
        train_predict = model.predict(X_train)
        train_predict = scaler.inverse_transform(train_predict)
        y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
        test_predict = model.predict(X_test)
        test_predict = scaler.inverse_transform(test_predict)
        y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))
        train_rmse = np.sqrt(mean_squared_error(y_train_actual, train_predict))
        test_rmse = np.sqrt(mean_squared_error(y_test_actual, test_predict))
        
        accuracy_history_callback.calculate_metrics(model)
        final_train_accuracy = accuracy_history_callback.train_accuracies[-1]
        final_val_accuracy = accuracy_history_callback.val_accuracies[-1]
        train_corr = accuracy_history_callback.train_correlations[-1]
        test_corr = accuracy_history_callback.val_correlations[-1]
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("è¨“ç·´é›†è¡¨ç¾")
            st.metric(label=f"RMSE", value=f"{train_rmse:.4f}")
            st.metric(label=f"ç›¸é—œä¿‚æ•¸ (R)", value=f"{train_corr:.4f}")
            st.metric(label=f"æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})", value=f"{final_train_accuracy:.2%}")
        with col2:
            st.subheader("æ¸¬è©¦/é©—è­‰é›†è¡¨ç¾")
            st.metric(label=f"RMSE", value=f"{test_rmse:.4f}")
            st.metric(label=f"ç›¸é—œä¿‚æ•¸ (R)", value=f"{test_corr:.4f}")
            st.metric(label=f"æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})", value=f"{final_val_accuracy:.2%}")

        st.subheader("æ¨¡å‹è¨“ç·´éç¨‹è©•ä¼°æ›²ç·š")
        if history_data:
            fig_loss_acc = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.08, 
                                         subplot_titles=("è¨“ç·´èˆ‡é©—è­‰æå¤± (MSE)", 
                                                         f"è¨“ç·´èˆ‡é©—è­‰æº–ç¢ºç‡ (Îµ={epsilon_value})", 
                                                         "è¨“ç·´èˆ‡é©—è­‰ç›¸é—œä¿‚æ•¸ (R)"))
            if 'loss' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['loss'], mode='lines', name='è¨“ç·´æå¤±'), row=1, col=1)
            if 'val_loss' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['val_loss'], mode='lines', name='é©—è­‰æå¤±'), row=1, col=1)
            if 'train_accuracy' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['train_accuracy'], mode='lines', name='è¨“ç·´æº–ç¢ºç‡'), row=2, col=1)
            if 'val_accuracy' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['val_accuracy'], mode='lines', name='é©—è­‰æº–ç¢ºç‡'), row=2, col=1)
            if 'train_correlation' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['train_correlation'], mode='lines', name='è¨“ç·´ç›¸é—œä¿‚æ•¸'), row=3, col=1)
            if 'val_correlation' in history_data:
                fig_loss_acc.add_trace(go.Scatter(y=history_data['val_correlation'], mode='lines', name='é©—è­‰ç›¸é—œä¿‚æ•¸'), row=3, col=1)
            
            fig_loss_acc.update_layout(height=800, hovermode="x unified")
            st.plotly_chart(fig_loss_acc, use_container_width=True, key="loss_chart")
        else:
            st.info("æ‰¾ä¸åˆ°èˆ‡æ­¤æ¨¡å‹é—œè¯çš„è¨“ç·´æ­·å²æ•¸æ“šã€‚")

        # --- é æ¸¬çµæœè¦–è¦ºåŒ–èˆ‡é¢¨éšªè©•ä¼°çš„æ­£ç¢ºé †åº ---

        st.subheader("ğŸ“ˆ é æ¸¬çµæœè¦–è¦ºåŒ–")
        
        # 1. å…ˆæº–å‚™å¥½æ‰€æœ‰ç¹ªåœ–éœ€è¦çš„ DataFrame
        last_sequence = scaled_data[-look_back:]
        future_predictions = []
        for _ in range(forecast_period_value):
            next_pred = model.predict(last_sequence.reshape(1, look_back, 1), verbose=0)[0, 0]
            future_predictions.append(next_pred)
            last_sequence = np.append(last_sequence[1:], [[next_pred]], axis=0)
        future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
        last_known_date = df_processed['ds'].max()
        
        if selected_freq_pandas in ['ME', 'YE']:
            future_start_date = last_known_date
        else:
            future_start_date = last_known_date + pd.to_timedelta(1, unit=selected_freq_pandas[0])

        future_dates = pd.date_range(start=future_start_date, periods=forecast_period_value, freq=selected_freq_pandas)
        forecast_df = pd.DataFrame({'ds': future_dates, 'yhat': future_predictions.flatten()})

        train_dates = df_processed['ds'][look_back : look_back + len(train_predict)].reset_index(drop=True)
        train_predict_df = pd.DataFrame({
            'ds': train_dates,
            'yhat_train': train_predict.flatten()
        })
        
        test_start_index = look_back + len(train_predict)
        test_end_index = test_start_index + len(test_predict)
        test_dates = df_processed['ds'][test_start_index : test_end_index].reset_index(drop=True)
        test_predict_df = pd.DataFrame({
            'ds': test_dates,
            'yhat_test': test_predict.flatten()
        })
        
        # 2. å»ºç«‹åœ–è¡¨ç‰©ä»¶ä¸¦ç•«åœ–
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=df_processed['ds'], y=df_processed['y'], mode='lines', name='å¯¦éš›æ•¸æ“š'))
        fig.add_trace(go.Scatter(x=train_predict_df['ds'], y=train_predict_df['yhat_train'], mode='lines', name='è¨“ç·´é›†é æ¸¬', line=dict(dash='dot')))
        fig.add_trace(go.Scatter(x=test_predict_df['ds'], y=test_predict_df['yhat_test'], mode='lines', name='æ¸¬è©¦é›†é æ¸¬', line=dict(dash='dot')))
        fig.add_trace(go.Scatter(x=forecast_df['ds'], y=forecast_df['yhat'], mode='lines', name='æœªä¾†é æ¸¬', line=dict(dash='dash')))
        fig.update_layout(title=f"{selected_station_name} - {selected_param_display} LSTM æœªä¾† {forecast_period_value} {selected_prediction_freq_display.split(' ')[0]} é æ¸¬", xaxis_title="æ™‚é–“", yaxis_title=f"{selected_param_display} {param_unit}", hovermode="x unified", height=600)
        st.plotly_chart(fig, use_container_width=True, key="forecast_chart")

        # 3. åœ¨åœ–è¡¨ä¸‹æ–¹ï¼Œé¡¯ç¤ºé¢¨éšªè©•ä¼°çµæœ
        st.write("---")
        st.subheader("è¿‘å²¸ä½œæ¥­é¢¨éšªè©•ä¼° (åŸºæ–¼é æ¸¬)")

        # <<< ä¿®æ­£é †åºï¼šç¬¬ä¸€æ­¥ï¼Œå…ˆæ‰¾å‡ºå°æ‡‰çš„ key >>>
        param_key_in_config = next((key for key, info in st.session_state.get('parameter_info', {}).items() 
                                    if info.get("display_zh") == selected_param_display), None)
        
        # <<< ä¿®æ­£é †åºï¼šç¬¬äºŒæ­¥ï¼Œç”¨é€™å€‹ key ç”¢ç”Ÿèªªæ˜æ–‡å­— >>>
        explanation_text = "æ­¤è©•ä¼°åŸºæ–¼ `config.json` ä¸­è¨­å®šçš„é¢¨éšªé–¾å€¼ã€‚"
        if param_key_in_config:
            thresholds = st.session_state.get('parameter_info', {}).get(param_key_in_config, {}).get("risk_thresholds", {})
            if thresholds:
                warning_level = thresholds.get("warning")
                danger_level = thresholds.get("danger")
                explanation_parts = []
                if warning_level is not None:
                    explanation_parts.append(f"**è­¦å‘Š**ç­‰ç´š > {warning_level} {param_unit}")
                if danger_level is not None:
                    explanation_parts.append(f"**å±éšª**ç­‰ç´š > {danger_level} {param_unit}")
                if explanation_parts:
                    explanation_text = f"æ­¤è©•ä¼°æ ¹æ“šç‚º **{selected_param_display}** è¨­å®šçš„é¢¨éšªé–¾å€¼ï¼š{'ï¼Œ'.join(explanation_parts)}ã€‚ç•¶æ‰€æœ‰é æ¸¬å€¼å‡ä½æ–¼ã€Œè­¦å‘Šã€æ¨™æº–æ™‚ï¼Œè©•ä¼°çµæœç‚ºå®‰å…¨ã€‚"
        
        st.info(explanation_text, icon="â„¹ï¸")

        # <<< ä¿®æ­£é †åºï¼šç¬¬ä¸‰æ­¥ï¼Œç”¨é€™å€‹ key åŸ·è¡Œé¢¨éšªè©•ä¼° >>>
        if param_key_in_config:
            forecast_df['risk_level'] = forecast_df['yhat'].apply(
                lambda value: assess_risk(value, param_key_in_config)
            )
        else:
            forecast_df['risk_level'] = "æœªçŸ¥"

        # é¡¯ç¤ºç¸½é«”é¢¨éšªçµè«–
        if "å±éšª" in forecast_df['risk_level'].unique():
            st.error(f"**ç¶œåˆè©•ä¼°ï¼šæœªä¾† {forecast_period_value} {selected_prediction_freq_display.split(' ')[0]} å…§å­˜åœ¨ã€Œå±éšªã€ç­‰ç´šé¢¨éšªï¼**")
        elif "è­¦å‘Š" in forecast_df['risk_level'].unique():
            st.warning(f"**ç¶œåˆè©•ä¼°ï¼šæœªä¾† {forecast_period_value} {selected_prediction_freq_display.split(' ')[0]} å…§å­˜åœ¨ã€Œè­¦å‘Šã€ç­‰ç´šé¢¨éšªã€‚**")
        else:
            st.success(f"**ç¶œåˆè©•ä¼°ï¼šæœªä¾† {forecast_period_value} {selected_prediction_freq_display.split(' ')[0]} å…§é æ¸¬å‡åœ¨å®‰å…¨ç¯„åœå…§ã€‚**")

        # é¡¯ç¤ºé«˜é¢¨éšªæ™‚æ®µè©³æƒ…
        risky_forecasts = forecast_df[forecast_df['risk_level'].isin(['è­¦å‘Š', 'å±éšª'])].copy()
        if not risky_forecasts.empty:
            with st.expander("æŸ¥çœ‹è©³ç´°é«˜é¢¨éšªæ™‚æ®µ"):
                risky_forecasts['ds'] = risky_forecasts['ds'].dt.strftime('%Y-%m-%d %H:%M')
                risky_forecasts['yhat'] = risky_forecasts['yhat'].map('{:.2f}'.format)
                st.dataframe(
                    risky_forecasts[['ds', 'yhat', 'risk_level']].rename(columns={
                        'ds': 'æ™‚é–“', 'yhat': f'é æ¸¬å€¼ ({param_unit})', 'risk_level': 'é¢¨éšªç­‰ç´š'
                    }),
                    use_container_width=True, hide_index=True
                )
        st.subheader("ğŸ’¾ ä¸‹è¼‰é æ¸¬çµæœèˆ‡å ±å‘Š")
        
        st.markdown("æ‚¨å¯ä»¥ä¸‹è¼‰åŒ…å«é æ¸¬å€¼çš„ CSV æ–‡ä»¶ã€äº’å‹•å¼åœ–è¡¨ï¼Œæˆ–ä¸€ä»½åŒ…å«æ‰€æœ‰åŸ·è¡Œåƒæ•¸èˆ‡çµæœçš„å®Œæ•´å ±å‘Šã€‚")

        col1, col2, col3 = st.columns(3)

        with col1:
            download_df = forecast_df.copy()
            download_df.rename(columns={'ds': 'æ™‚é–“', 'yhat': f'é æ¸¬å€¼_{selected_param_display}'}, inplace=True)
            download_df['æ™‚é–“'] = download_df['æ™‚é–“'].dt.strftime('%Y-%m-%d %H:%M:%S')
            csv_data = download_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ä¸‹è¼‰é æ¸¬æ•¸æ“š (CSV)",
                data=csv_data,
                file_name=f"{selected_station_name}_{selected_param_col}_LSTM_forecast_data.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            fig_data = fig.to_html(full_html=True, include_plotlyjs='cdn')
            fig_bytes = fig_data.encode('utf-8')

            st.download_button(
                label="ä¸‹è¼‰é æ¸¬åœ–è¡¨ (HTML)",
                data=fig_bytes,
                file_name=f"{selected_station_name}_{selected_param_col}_LSTM_forecast_chart.html",
                mime="text/html",
                use_container_width=True
            )
            
        with col3:
        # <<< ç°¡åŒ–ä¸¦ä¿®æ­£åˆ¤æ–·é‚è¼¯ >>>
            actual_epochs_text = f"(å¯¦éš›åŸ·è¡Œ: {len(history_data['loss'])})" if history_data else "(å¾å¿«å–è¼‰å…¥)"

            report_content = f"""
    # LSTM æ™‚é–“åºåˆ—é æ¸¬å ±å‘Š
    ## æ¸¬ç«™: {selected_station_name}
    ## é æ¸¬åƒæ•¸: {selected_param_display} ({param_unit})

    ---
    ## 1. æ•¸æ“šæ¦‚è¦½èˆ‡è¨­å®š
    - **ä½¿ç”¨æ•¸æ“šå€é–“**: {df_processed['ds'].min().strftime('%Y-%m-%d %H:%M')} åˆ° {df_processed['ds'].max().strftime('%Y-%m-%d %H:%M')}
    - **ç¸½æ™‚é•·**: {df_processed['ds'].max() - df_processed['ds'].min()}
    - **ç¸½ç­†æ•¸ (é è™•ç†å¾Œ)**: {len(df_processed)} ç­†
    - **é æ¸¬é »æ¬¡**: {selected_prediction_freq_display}

    ---
    ## 2. æ•¸æ“šé è™•ç†è¨­å®š
    - **ç¼ºå¤±å€¼è™•ç†ç­–ç•¥**: {missing_value_strategy}
    - **æ•¸æ“šå¹³æ»‘è™•ç†**: {'æ˜¯' if apply_smoothing and smoothing_window > 1 else 'å¦'}
    """
            if apply_smoothing and smoothing_window > 1:
                report_content += f"   - **ç§»å‹•å¹³å‡è¦–çª—**: {smoothing_window}\n"

            report_content += f"""
    ---
    ## 3. LSTM æ¨¡å‹åƒæ•¸
    - **å›æº¯æ™‚é–“æ­¥ (look_back)**: {look_back}
    - **LSTM å±¤å–®å…ƒæ•¸**: {lstm_units}
    - **è¨“ç·´è¿­ä»£æ¬¡æ•¸ (Epochs)**: {epochs} {actual_epochs_text}
    - **æ‰¹æ¬¡å¤§å° (Batch Size)**: {batch_size}
    - **Dropout æ¯”ç‡**: {dropout_rate}
    - **é©—è­‰é›†æ¯”ä¾‹**: {validation_split:.2f}
    - **æ—©åœè€å¿ƒå€¼ (Patience)**: {patience}

    ---
    ## 4. æ¨¡å‹æ€§èƒ½è©•ä¼° (æœ€çµ‚)
    ### è¨“ç·´é›†è¡¨ç¾
    - **RMSE**: {train_rmse:.4f}
    - **ç›¸é—œä¿‚æ•¸ (R)**: {train_corr:.4f}
    - **æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})**: {final_train_accuracy:.2%}

    ### æ¸¬è©¦/é©—è­‰é›†è¡¨ç¾
    - **RMSE**: {test_rmse:.4f}
    - **ç›¸é—œä¿‚æ•¸ (R)**: {test_corr:.4f}
    - **æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})**: {final_val_accuracy:.2%}

    ---
    å ±å‘Šç”Ÿæˆæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
    """
            st.download_button(
                label="ä¸‹è¼‰å®Œæ•´å ±å‘Š (TXT)",
                data=report_content.encode('utf-8'),
                file_name=f"{selected_station_name}_{selected_param_col}_LSTM_report.txt",
                mime="text/plain",
                use_container_width=True,
                help="ä¸‹è¼‰åŒ…å«æ‰€æœ‰åŸ·è¡Œåƒæ•¸èˆ‡çµæœçš„æ–‡æœ¬å ±å‘Š"
            )

        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
            zf.writestr(f"{selected_station_name}_{selected_param_col}_LSTM_forecast_data.csv", csv_data)
            zf.writestr(f"{selected_station_name}_{selected_param_col}_LSTM_report.txt", report_content.encode('utf-8'))
            zf.writestr(f"{selected_station_name}_{selected_param_col}_LSTM_forecast_chart.html", fig_bytes)
        st.download_button("ğŸš€ ä¸€éµæ‰“åŒ…ä¸‹è¼‰ (ZIP)", zip_buffer.getvalue(), f"{selected_station_name}_{selected_param_col}_LSTM_forecast_package.zip",
                           "application/zip", use_container_width=True,
                           help="ä¸‹è¼‰åŒ…å«é æ¸¬æ•¸æ“šã€åœ–è¡¨å’Œå ±å‘Šçš„å£“ç¸®åŒ…")
