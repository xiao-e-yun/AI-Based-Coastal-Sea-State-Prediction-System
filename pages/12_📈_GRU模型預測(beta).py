import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Dense, Dropout, GRU 
from keras.callbacks import EarlyStopping, Callback
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# å¾ helpers æ¨¡çµ„å°å…¥æ‰€æœ‰å¿…è¦çš„é€šç”¨å‡½æ•¸å’Œå…¨å±€è®Šæ•¸
# å‡è¨­ helpers.py ä¸­æœ‰é€™äº›å‡½æ•¸
from utils.helpers import (
    get_station_name_from_id,
    initialize_session_state,
    load_app_config_and_font, 
    load_data_for_prediction_page, 
    create_sequences, 
    PARAMETER_INFO, 
    BASE_DATA_PATH_FROM_CONFIG,
    CHINESE_FONT_NAME,
    load_year_data,
    get_available_years 
)

# è¨­ç½® TensorFlow æ—¥èªŒç´šåˆ¥ï¼ŒæŠ‘åˆ¶ INFO è¨Šæ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 

# --- Streamlit é é¢è¨­å®š ---
st.set_page_config(
    page_title="GRU æ¨¡å‹é æ¸¬",
    page_icon="ğŸ“ˆ",
    layout="wide"
)
initialize_session_state()

st.title("ğŸ“ˆ æµ·æ´‹æ•¸æ“š GRU æ¨¡å‹é æ¸¬")
st.markdown("ä½¿ç”¨é–€æ§å¾ªç’°å–®å…ƒ (GRU) é¡ç¥ç¶“ç¶²çµ¡é æ¸¬æµ·æ´‹æ•¸æ“šçš„æœªä¾†è¶¨å‹¢ã€‚")

# --- è¼‰å…¥é…ç½® ---
try:
    app_config = load_app_config_and_font()
    STATION_COORDS = app_config.get("STATION_COORDS", {})
except Exception as e:
    st.error(f"ç„¡æ³•è¼‰å…¥æ‡‰ç”¨ç¨‹å¼é…ç½®ï¼š{e}")
    st.stop()

# --- GRU æ¨¡å‹è¼”åŠ©å‡½æ•¸ ---
def build_gru_model(input_shape, gru_units, dense_units, num_gru_layers, dropout=0.2):
    inputs = Input(shape=input_shape)
    x = inputs
    for i in range(num_gru_layers):
        return_sequences = True if i < num_gru_layers - 1 else False
        x = GRU(gru_units, return_sequences=return_sequences, dropout=dropout)(x)
    for dim in dense_units:
        x = Dense(dim, activation="relu")(x)
        x = Dropout(dropout)(x)
    outputs = Dense(1)(x)
    return Model(inputs=inputs, outputs=outputs)


# --- å´é‚Šæ¬„ï¼šGRU é æ¸¬è¨­å®šæ§åˆ¶é … ---
st.sidebar.header("GRU é æ¸¬è¨­å®š")

locations = st.session_state.get('locations', [])
if not locations:
    st.sidebar.warning("è«‹åœ¨ `config.json` çš„ `STATION_COORDS` ä¸­é…ç½®æ¸¬ç«™è³‡è¨Šã€‚")
    st.stop()

selected_station = st.sidebar.selectbox("é¸æ“‡æ¸¬ç«™:", locations, key='pages_12_gru_station', format_func=get_station_name_from_id)
selected_station_name = get_station_name_from_id(selected_station)

predictable_params_config_map = {
    col_name: info["display_zh"] for col_name, info in PARAMETER_INFO.items()
    if info.get("type") == "linear"
}

# å‹•æ…‹ç²å–å¯ç”¨åƒæ•¸
available_predictable_params_display_to_col = {}
if selected_station:
    current_year = pd.Timestamp.now().year
    temp_base_path = os.path.normpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', BASE_DATA_PATH_FROM_CONFIG))
    temp_df_for_col_check = None
    for y_check in range(current_year, current_year - 5, -1):
        temp_df_for_col_check = load_year_data(temp_base_path, selected_station, y_check)
        if temp_df_for_col_check is not None and not temp_df_for_col_check.empty: break
    
    if temp_df_for_col_check is not None and not temp_df_for_col_check.empty:
        for col_name, display_name in predictable_params_config_map.items():
            if col_name in temp_df_for_col_check.columns and pd.api.types.is_numeric_dtype(temp_df_for_col_check[col_name]):
                available_predictable_params_display_to_col[display_name] = col_name
    else:
        st.sidebar.warning(f"ç„¡æ³•ç‚ºæ¸¬ç«™ '{selected_station_name}' è¼‰å…¥ä»»ä½•æ­·å²æ•¸æ“šä»¥ç¢ºèªå¯ç”¨åƒæ•¸ã€‚è«‹æª¢æŸ¥æ•¸æ“šæª”æ¡ˆã€‚")

if not available_predictable_params_display_to_col:
    st.sidebar.error("æ²’æœ‰å¯ä¾›é æ¸¬çš„æœ‰æ•ˆæ•¸å€¼å‹åƒæ•¸ã€‚")
    st.stop()

selected_param_display = st.sidebar.selectbox("é¸æ“‡é æ¸¬åƒæ•¸:", list(available_predictable_params_display_to_col.keys()), key='pages_12_gru_param_display')
selected_param_col = available_predictable_params_display_to_col[selected_param_display]
param_info_original = PARAMETER_INFO.get(selected_param_col, {})
selected_param_display_original = param_info_original.get("display_zh", selected_param_col)
param_unit = param_info_original.get("unit", "")

st.sidebar.markdown("---")
st.sidebar.subheader("é æ¸¬æ™‚é–“è¨­å®š")
# ä¿®æ”¹ prediction_frequencies å­—å…¸ï¼Œä½¿ç”¨ 'ME' å’Œ 'YE'
prediction_frequencies = {"å°æ™‚ (H)": "h", "å¤© (D)": "D", "é€± (W)": "W", "æœˆ (M)": "ME", "å¹´ (Y)": "YE"}
selected_prediction_freq_display = st.sidebar.selectbox("é¸æ“‡é æ¸¬é »æ¬¡:", list(prediction_frequencies.keys()), key='pages_12_prediction_frequency')
selected_freq_pandas = prediction_frequencies[selected_prediction_freq_display]

# æ ¹æ“šæ–°çš„é »ç‡åˆ¥åèª¿æ•´ max_forecast_value_map
max_forecast_value_map = {'h': 24 * 30, 'D': 365, 'W': 104, 'ME': 24, 'YE': 5} # ä½¿ç”¨ 'ME' å’Œ 'YE'
max_forecast_value = max_forecast_value_map.get(selected_freq_pandas, 30)
default_forecast_value = 24 if selected_freq_pandas == 'h' else 7 if selected_freq_pandas == 'D' else 4 if selected_freq_pandas == 'ME' else 1 # èª¿æ•´é è¨­å€¼

forecast_period_value = st.sidebar.number_input(f"é æ¸¬æœªä¾†å¤šä¹… ({selected_prediction_freq_display.split(' ')[0]}):", 1, max_forecast_value, min(default_forecast_value, max_forecast_value), 1, key='pages_12_forecast_period_value')

st.sidebar.markdown("---")
st.sidebar.subheader("è¨“ç·´æ•¸æ“šæ™‚é–“ç¯„åœ")
available_years = get_available_years(BASE_DATA_PATH_FROM_CONFIG, locations)
if not available_years:
    st.sidebar.error("æ²’æœ‰å¯ç”¨çš„æ•¸æ“šå¹´ä»½ã€‚")
    st.stop()
min_year_available, max_year_available = min(available_years), max(available_years)
min_date_available, max_date_available = pd.to_datetime(f'{min_year_available}-01-01').date(), pd.to_datetime(f'{max_year_available}-12-31').date()
default_start_date = max(min_date_available, max_date_available - pd.Timedelta(days=365))
train_start_date = st.sidebar.date_input("è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸ:", default_start_date, min_date_available, max_date_available, key='pages_12_train_start_date')
train_end_date = st.sidebar.date_input("è¨“ç·´æ•¸æ“šçµæŸæ—¥æœŸ:", max_date_available, min_date_available, max_date_available, key='pages_12_train_end_date')
if train_start_date >= train_end_date:
    st.sidebar.error("è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸå¿…é ˆæ—©æ–¼çµæŸæ—¥æœŸã€‚")
    st.stop()

st.sidebar.markdown("---")
st.sidebar.subheader("æ•¸æ“šé è™•ç†")
missing_value_strategy = st.sidebar.selectbox("ç¼ºå¤±å€¼è™•ç†:", ['å‰å‘å¡«å…… (ffill)', 'å¾Œå‘å¡«å…… (bfill)', 'ç·šæ€§æ’å€¼ (interpolate)', 'ç§»é™¤ç¼ºå¤±å€¼ (dropna)'], key='pages_12_missing_strategy')
apply_smoothing = st.sidebar.checkbox("æ‡‰ç”¨æ•¸æ“šå¹³æ»‘", False, key='pages_12_apply_smoothing')
if apply_smoothing:
    smoothing_window = st.sidebar.slider("å¹³æ»‘è™•ç† (ç§»å‹•å¹³å‡è¦–çª—):", 1, 24, 3, 1, help="ç§»å‹•å¹³å‡è¦–çª—å¤§å°ã€‚", key='pages_12_smoothing_window')
else:
    smoothing_window = 1

st.sidebar.markdown("---")
st.sidebar.subheader("æ•¸æ“šæ­£è¦åŒ–")
normalization_method = st.sidebar.selectbox("é¸æ“‡æ­£è¦åŒ–æ–¹æ³•:", ['Min-Max æ­¸ä¸€åŒ– (0-1)', 'æ¨™æº–åŒ– (Z-score)', 'RobustScaler (ä¸­ä½æ•¸-å››åˆ†ä½è·)'], key='pages_12_normalization_method')

st.sidebar.markdown("---")
st.sidebar.subheader("æ¨¡å‹æº–ç¢ºç‡è¨­å®š")
epsilon_value = st.sidebar.number_input("æº–ç¢ºç‡ Îµ èª¤å·®å€é–“:", 0.001, 10.0, 0.1, 0.01, "%.3f", help="è¨­å®šä¸€å€‹èª¤å·®ç¯„åœ Îµã€‚ç•¶ |é æ¸¬å€¼ - å¯¦éš›å€¼| <= Îµ æ™‚ï¼Œæ­¤é æ¸¬è¢«è¦–ç‚ºã€Œæ­£ç¢ºã€ã€‚", key='pages_12_epsilon_value')

st.sidebar.markdown("---")
st.sidebar.subheader("GRU æ¨¡å‹åƒæ•¸")
look_back = st.sidebar.slider("å›æº¯æ™‚é–“æ­¥ (look_back):", 1, 48, 12, 1, help="GRU æ¨¡å‹è€ƒæ…®å¤šå°‘å€‹éå»çš„æ™‚é–“é»ã€‚", key='pages_12_gru_look_back')
gru_units = st.sidebar.slider("GRU å±¤å–®å…ƒæ•¸:", 32, 256, 64, 32, help="GRU å±¤çš„ç¥ç¶“å…ƒæ•¸é‡ã€‚", key='pages_12_gru_units')
num_gru_layers = st.sidebar.slider("GRU å±¤æ•¸é‡:", 1, 5, 2, 1, help="å †ç–Šçš„ GRU å±¤æ•¸é‡ã€‚", key='pages_12_num_gru_layers')
mlp_units = st.sidebar.multiselect("MLP å±¤å–®å…ƒæ•¸:", [32, 64, 128, 256], [128], help="é æ¸¬é ­éƒ¨çš„å¤šå±¤æ„ŸçŸ¥å™¨å±¤ã€‚", key='pages_12_mlp_units')
epochs = st.sidebar.number_input("è¨“ç·´è¿­ä»£æ¬¡æ•¸ (Epochs):", 10, 500, 100, 10, key='pages_12_epochs')
batch_size = st.sidebar.number_input("æ‰¹æ¬¡å¤§å° (Batch Size):", 1, 128, 32, 8, key='pages_12_batch_size')
dropout_rate = st.sidebar.slider("Dropout æ¯”ç‡:", 0.0, 0.5, 0.2, 0.05, help="é˜²æ­¢éæ“¬åˆã€‚", key='pages_12_dropout_rate')
validation_split = st.sidebar.slider("é©—è­‰é›†æ¯”ä¾‹:", 0.0, 0.5, 0.1, 0.05, help="ç”¨æ–¼æ¨¡å‹é©—è­‰çš„æ•¸æ“šæ¯”ä¾‹ã€‚", key='pages_12_validation_split')
# ä¿®æ”¹æ—©åœè€å¿ƒå€¼çš„ max_value å’Œ value
patience = st.sidebar.number_input("æ—©åœè€å¿ƒå€¼ (Patience):", min_value=5, max_value=200, value=50, step=5, help="é©—è­‰æå¤±åœ¨å¤šå°‘å€‹ epochs å…§æ²’æœ‰æ”¹å–„å‰‡åœæ­¢è¨“ç·´ã€‚", key='pages_12_patience')

# --- ä¿®æ”¹ï¼šå¢å¼· AccuracyHistory ä»¥åŒ…å«ç›¸é—œä¿‚æ•¸ ---
class AccuracyHistory(Callback):
    def __init__(self, X_train, y_train, X_test, y_test, scaler, epsilon):
        super().__init__()
        self.X_train, self.y_train = X_train, y_train
        self.X_test, self.y_test = X_test, y_test
        self.scaler, self.epsilon = scaler, epsilon
        self.train_accuracies, self.val_accuracies = [], []
        self.train_correlations, self.val_correlations = [], [] # æ–°å¢
    
    def on_epoch_end(self, epoch, logs=None):
        # è¨“ç·´é›†
        train_pred_scaled = self.model.predict(self.X_train, verbose=0)
        train_actual_scaled = self.y_train.reshape(-1, 1)
        train_pred_original = self.scaler.inverse_transform(train_pred_scaled)
        train_actual_original = self.scaler.inverse_transform(train_actual_scaled)
        self.train_accuracies.append(np.sum(np.abs(train_pred_original - train_actual_original) <= self.epsilon) / len(train_actual_original))
        if len(train_actual_original) > 1:
            train_corr, _ = pearsonr(train_actual_original.flatten(), train_pred_original.flatten())
            self.train_correlations.append(train_corr)
        else:
            self.train_correlations.append(np.nan)
        
        # é©—è­‰é›†
        val_pred_scaled = self.model.predict(self.X_test, verbose=0)
        val_actual_scaled = self.y_test.reshape(-1, 1)
        val_pred_original = self.scaler.inverse_transform(val_pred_scaled)
        val_actual_original = self.scaler.inverse_transform(val_actual_scaled)
        self.val_accuracies.append(np.sum(np.abs(val_pred_original - val_actual_original) <= self.epsilon) / len(val_actual_original))
        if len(val_actual_original) > 1:
            val_corr, _ = pearsonr(val_actual_original.flatten(), val_pred_original.flatten())
            self.val_correlations.append(val_corr)
        else:
            self.val_correlations.append(np.nan)

# --- åŸ·è¡Œé æ¸¬æŒ‰éˆ• ---
if st.sidebar.button("ğŸ“ˆ åŸ·è¡Œ GRU é æ¸¬"):
    if not tf.config.list_physical_devices('GPU'):
        st.warning("è­¦å‘Š: TensorFlow æœªå•Ÿç”¨ GPU åŠ é€Ÿã€‚æ¨¡å‹è¨“ç·´å¯èƒ½è¼ƒæ…¢ã€‚")

    with st.spinner("æ­£åœ¨è¼‰å…¥æ•¸æ“š..."):
        df_raw = load_data_for_prediction_page(selected_station, selected_param_col, train_start_date, train_end_date)
    
    if df_raw.empty:
        st.error(f"ç„¡æ³•ç‚ºæ¸¬ç«™ '{selected_station_name}' åœ¨æŒ‡å®šæ™‚é–“ç¯„åœå…§è¼‰å…¥åƒæ•¸ '{selected_param_display_original}' çš„æ•¸æ“šã€‚")
        st.stop()
    
    st.info(f"æ­£åœ¨å°æ¸¬ç«™ **{selected_station_name}** çš„åƒæ•¸ **{selected_param_display_original}** åŸ·è¡Œ GRU é æ¸¬...")

    # --- æ•¸æ“šé è™•ç† ---
    df_processed = df_raw.copy().sort_values('ds').drop_duplicates(subset=['ds'], keep='first')
    df_processed = df_processed.set_index('ds').resample(selected_freq_pandas).mean()
    if missing_value_strategy == 'å‰å‘å¡«å…… (ffill)': df_processed['y'] = df_processed['y'].ffill()
    elif missing_value_strategy == 'å¾Œå‘å¡«å…… (bfill)': df_processed['y'] = df_processed['y'].bfill()
    elif missing_value_strategy == 'ç·šæ€§æ’å€¼ (interpolate)': df_processed['y'] = df_processed['y'].interpolate(method='linear')
    elif missing_value_strategy == 'ç§»é™¤ç¼ºå¤±å€¼ (dropna)': df_processed.dropna(subset=['y'], inplace=True)
    if df_processed['y'].isnull().all(): st.error("æ•¸æ“šé è™•ç†å¾Œå…¨éƒ¨ç‚ºç©ºå€¼ã€‚"); st.stop()
    if apply_smoothing and smoothing_window > 1: df_processed['y'] = df_processed['y'].rolling(smoothing_window, min_periods=1, center=True).mean()
    df_processed.dropna(subset=['y'], inplace=True)
    df_processed.reset_index(inplace=True) 
    if len(df_processed) <= look_back: st.error(f"æœ‰æ•ˆæ•¸æ“šé» ({len(df_processed)}) ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ (éœ€è¦ > {look_back})ã€‚"); st.stop()
    
    # --- æ­£è¦åŒ–èˆ‡åºåˆ—å‰µå»º ---
    if normalization_method.startswith('Min-Max'): scaler = MinMaxScaler(feature_range=(0, 1))
    elif normalization_method.startswith('æ¨™æº–åŒ–'): scaler = StandardScaler()
    else: scaler = RobustScaler()
    scaled_data = scaler.fit_transform(df_processed['y'].values.reshape(-1, 1))
    X, y = create_sequences(scaled_data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1)) 
    train_size = int(len(X) * (1 - validation_split))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # --- æ¨¡å‹è¨“ç·´ ---
    with st.spinner("æ­£åœ¨å»ºç«‹ä¸¦è¨“ç·´ GRU æ¨¡å‹..."):
        model = build_gru_model((look_back, 1), gru_units, mlp_units, num_gru_layers, dropout_rate)
        model.compile(optimizer='adam', loss='mean_squared_error')
        early_stopping = EarlyStopping('val_loss', patience=patience, restore_best_weights=True)
        accuracy_history_callback = AccuracyHistory(X_train, y_train, X_test, y_test, scaler, epsilon_value)
        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping, accuracy_history_callback], verbose=0)
    st.success("GRU æ¨¡å‹è¨“ç·´å®Œæˆï¼")

    # --- æ•¸æ“šæ¦‚è¦½èˆ‡å“è³ªå ±å‘Š ---
    st.subheader("ğŸ“Š æ•¸æ“šæ¦‚è¦½èˆ‡å“è³ªåˆ†æ")
    # ... (é€™éƒ¨åˆ†å¯ä»¥å¾æ‚¨çš„ LSTM ç¨‹å¼ç¢¼ä¸­è¤‡è£½éä¾†ï¼Œé€™è£¡æš«æ™‚çœç•¥ä»¥ä¿æŒç°¡æ½”)

    # --- ä¿®æ”¹ï¼šé‡æ§‹æ¨¡å‹æ€§èƒ½è©•ä¼°å€å¡Š ---
    st.subheader("ğŸ“‰ æ¨¡å‹æ€§èƒ½è©•ä¼°")
    train_predict = scaler.inverse_transform(model.predict(X_train, verbose=0))
    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
    test_predict = scaler.inverse_transform(model.predict(X_test, verbose=0))
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))
    train_rmse = np.sqrt(mean_squared_error(y_train_actual, train_predict))
    test_rmse = np.sqrt(mean_squared_error(y_test_actual, test_predict))
    train_corr, _ = pearsonr(y_train_actual.flatten(), train_predict.flatten()) if len(y_train_actual) > 1 else (np.nan, np.nan)
    test_corr, _ = pearsonr(y_test_actual.flatten(), test_predict.flatten()) if len(y_test_actual) > 1 else (np.nan, np.nan)
    final_train_accuracy = accuracy_history_callback.train_accuracies[-1] if accuracy_history_callback.train_accuracies else np.nan
    final_val_accuracy = accuracy_history_callback.val_accuracies[-1] if accuracy_history_callback.val_accuracies else np.nan
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("è¨“ç·´é›†è¡¨ç¾")
        st.metric("RMSE", f"{train_rmse:.4f}", help="å‡æ–¹æ ¹èª¤å·®ï¼Œå€¼è¶Šå°è¶Šå¥½ã€‚")
        st.metric("ç›¸é—œä¿‚æ•¸ (R)", f"{train_corr:.4f}", help="è¡¡é‡è¶¨å‹¢å»åˆåº¦ï¼Œå€¼è¶Šæ¥è¿‘ 1 è¶Šå¥½ã€‚")
        st.metric(f"æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})", f"{final_train_accuracy:.2%}", help=f"åœ¨ Â±{epsilon_value:.2f} èª¤å·®å…§çš„æ­£ç¢ºç‡ã€‚")
    with col2:
        st.subheader("æ¸¬è©¦/é©—è­‰é›†è¡¨ç¾")
        st.metric("RMSE", f"{test_rmse:.4f}", help="å°æœªè¦‹æ•¸æ“šçš„æ³›åŒ–èª¤å·®ï¼Œå€¼è¶Šå°è¶Šå¥½ã€‚")
        st.metric("ç›¸é—œä¿‚æ•¸ (R)", f"{test_corr:.4f}", help="å°æœªè¦‹æ•¸æ“šçš„è¶¨å‹¢å»åˆåº¦ï¼Œå€¼è¶Šæ¥è¿‘ 1 è¶Šå¥½ã€‚")
        st.metric(f"æº–ç¢ºç‡ (Îµ={epsilon_value:.2f})", f"{final_val_accuracy:.2%}", help=f"åœ¨ Â±{epsilon_value:.2f} èª¤å·®å…§çš„æ­£ç¢ºç‡ã€‚")

    # --- ä¿®æ”¹ï¼šå¢å¼·è¨“ç·´éç¨‹åœ–è¡¨ ---
    st.subheader("ğŸ“ˆ æ¨¡å‹è¨“ç·´éç¨‹è©•ä¼°æ›²ç·š")
    fig_loss_acc = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.08, subplot_titles=("è¨“ç·´èˆ‡é©—è­‰æå¤± (MSE)", f"è¨“ç·´èˆ‡é©—è­‰æº–ç¢ºç‡ (Îµ={epsilon_value})", "è¨“ç·´èˆ‡é©—è­‰ç›¸é—œä¿‚æ•¸ (R)"))
    fig_loss_acc.add_trace(go.Scatter(y=history.history['loss'], name='è¨“ç·´æå¤±'), row=1, col=1)
    if 'val_loss' in history.history: fig_loss_acc.add_trace(go.Scatter(y=history.history['val_loss'], name='é©—è­‰æå¤±'), row=1, col=1)
    fig_loss_acc.add_trace(go.Scatter(y=accuracy_history_callback.train_accuracies, name='è¨“ç·´æº–ç¢ºç‡', line=dict(color='green')), row=2, col=1)
    fig_loss_acc.add_trace(go.Scatter(y=accuracy_history_callback.val_accuracies, name='é©—è­‰æº–ç¢ºç‡', line=dict(color='red')), row=2, col=1)
    fig_loss_acc.add_trace(go.Scatter(y=accuracy_history_callback.train_correlations, name='è¨“ç·´ç›¸é—œä¿‚æ•¸', line=dict(color='purple')), row=3, col=1)
    fig_loss_acc.add_trace(go.Scatter(y=accuracy_history_callback.val_correlations, name='é©—è­‰ç›¸é—œä¿‚æ•¸', line=dict(color='cyan')), row=3, col=1)
    fig_loss_acc.update_layout(height=800, xaxis_title="Epoch", yaxis1=dict(title="æå¤± (MSE)"), yaxis2=dict(title="æº–ç¢ºç‡"), yaxis3=dict(title="ç›¸é—œä¿‚æ•¸ (R)"), hovermode="x unified", font=dict(family=CHINESE_FONT_NAME))
    st.plotly_chart(fig_loss_acc, use_container_width=True)

    # --- é æ¸¬çµæœè¦–è¦ºåŒ– ---
    st.subheader("ğŸ“Š æœªä¾†è¶¨å‹¢é æ¸¬")
    last_sequence = scaled_data[-look_back:]
    future_predictions = []
    for _ in range(forecast_period_value):
        next_pred = model.predict(last_sequence.reshape(1, look_back, 1), verbose=0)[0, 0]
        future_predictions.append(next_pred)
        last_sequence = np.append(last_sequence[1:], [[next_pred]], axis=0)
    future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
    last_known_date = df_processed['ds'].max()

    # Determine the start date for the future prediction range based on frequency type
    # Check for calendar-based frequencies (Month End, Year End)
    if selected_freq_pandas in ['ME', 'YE']: # <-- ä¿®æ”¹åˆ¤æ–·æ¢ä»¶ï¼Œæª¢æŸ¥ 'ME' å’Œ 'YE'
        # For monthly or yearly frequencies, pd.date_range starts *after* the start date
        # So, we just use the last known date as the start reference.
        # pd.date_range with freq='ME' or 'YE' will correctly generate dates at the end of each period.
        future_start_date = last_known_date
    else:
        # For fixed frequencies (hourly, daily, weekly), add one unit to get the next point
        # This is where pd.to_timedelta is appropriate
        future_start_date = last_known_date + pd.to_timedelta(1, unit=selected_freq_pandas)

    future_dates = pd.date_range(start=future_start_date, periods=forecast_period_value, freq=selected_freq_pandas)
    forecast_df = pd.DataFrame({'ds': future_dates, 'yhat': future_predictions.flatten()})

    # æº–å‚™ç¹ªåœ–æ•¸æ“š
    full_plot_df = df_processed.copy()
    full_plot_df['yhat_train'] = np.nan
    full_plot_df.loc[df_processed.index[look_back:len(train_predict) + look_back], 'yhat_train'] = train_predict.flatten()
    full_plot_df['yhat_test'] = np.nan
    full_plot_df.loc[df_processed.index[len(train_predict) + look_back:], 'yhat_test'] = test_predict.flatten()
    
    # ç¹ªåœ–
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=full_plot_df['ds'], y=full_plot_df['y'], name='å¯¦éš›æ•¸æ“š', line=dict(color='blue')))
    fig.add_trace(go.Scatter(x=full_plot_df['ds'], y=full_plot_df['yhat_train'], name='è¨“ç·´é›†é æ¸¬', line=dict(color='green', dash='dot')))
    fig.add_trace(go.Scatter(x=full_plot_df['ds'], y=full_plot_df['yhat_test'], name='æ¸¬è©¦é›†é æ¸¬', line=dict(color='orange', dash='dot')))
    fig.add_trace(go.Scatter(x=forecast_df['ds'], y=forecast_df['yhat'], name='æœªä¾†é æ¸¬', line=dict(color='red', dash='dash')))
    fig.update_layout(title=f"{selected_station_name} - {selected_param_display_original} GRU é æ¸¬", xaxis_title="æ™‚é–“", yaxis_title=f"{selected_param_display_original} {param_unit}", height=600, font=dict(family=CHINESE_FONT_NAME))
    st.plotly_chart(fig, use_container_width=True)

    # --- ä¿®æ”¹ï¼šæ“´å……ä¸‹è¼‰åŠŸèƒ½ ---
    st.subheader("ğŸ’¾ ä¸‹è¼‰é æ¸¬çµæœèˆ‡å ±å‘Š")
    st.markdown("æ‚¨å¯ä»¥ä¸‹è¼‰é æ¸¬æ•¸æ“šã€äº’å‹•å¼åœ–è¡¨æˆ–ä¸€ä»½åŒ…å«æ‰€æœ‰åŸ·è¡Œåƒæ•¸èˆ‡çµæœçš„å®Œæ•´å ±å‘Šã€‚")
    col1, col2, col3 = st.columns(3)
    with col1:
        csv_data = forecast_df.rename(columns={'ds': 'æ™‚é–“', 'yhat': f'é æ¸¬å€¼_{selected_param_display_original}'}).to_csv(index=False).encode('utf-8')
        st.download_button("ä¸‹è¼‰é æ¸¬æ•¸æ“š (CSV)", csv_data, f"{selected_station_name}_{selected_param_col}_GRU_data.csv", "text/csv", use_container_width=True)
    with col2:
        html_bytes = fig.to_html(full_html=True, include_plotlyjs='cdn').encode('utf-8')
        st.download_button("ä¸‹è¼‰é æ¸¬åœ–è¡¨ (HTML)", html_bytes, f"{selected_station_name}_{selected_param_col}_GRU_chart.html", "text/html", use_container_width=True)
    with col3:
        report_content = f"""
# GRU æ™‚é–“åºåˆ—é æ¸¬å ±å‘Š
## æ¸¬ç«™: {selected_station_name} | é æ¸¬åƒæ•¸: {selected_param_display_original} ({param_unit})
---
## 1. æ•¸æ“šèˆ‡é æ¸¬è¨­å®š
- æ•¸æ“šå€é–“: {train_start_date.strftime('%Y-%m-%d')} åˆ° {train_end_date.strftime('%Y-%m-%d')}
- é æ¸¬é »æ¬¡: {selected_prediction_freq_display}
- é æ¸¬æœªä¾†æ™‚é•·: {forecast_period_value} {selected_prediction_freq_display.split(' ')[0]}
- ç¼ºå¤±å€¼è™•ç†: {missing_value_strategy}
- æ•¸æ“šå¹³æ»‘: {'æ˜¯ (çª—å£: ' + str(smoothing_window) + ')' if apply_smoothing and smoothing_window > 1 else 'å¦'}
- æ­£è¦åŒ–æ–¹æ³•: {normalization_method}
---
## 2. GRU æ¨¡å‹åƒæ•¸
- å›æº¯æ™‚é–“æ­¥ (look_back): {look_back}
- GRU å±¤æ•¸é‡: {num_gru_layers}
- GRU å±¤å–®å…ƒæ•¸: {gru_units}
- MLP å±¤å–®å…ƒæ•¸: {mlp_units}
- è¨“ç·´è¿­ä»£æ¬¡æ•¸ (Epochs): {epochs} (å¯¦éš›åŸ·è¡Œ: {len(history.history['loss'])})
- æ‰¹æ¬¡å¤§å°: {batch_size}
- Dropout æ¯”ç‡: {dropout_rate}
- é©—è­‰é›†æ¯”ä¾‹: {validation_split:.2f}
- æ—©åœè€å¿ƒå€¼: {patience}
---
## 3. æ¨¡å‹æ€§èƒ½è©•ä¼°
### è¨“ç·´é›†è¡¨ç¾
- RMSE: {train_rmse:.4f}
- ç›¸é—œä¿‚æ•¸ (R): {train_corr:.4f}
- æº–ç¢ºç‡ (Îµ={epsilon_value:.2f}): {final_train_accuracy:.2%}
### æ¸¬è©¦/é©—è­‰é›†è¡¨ç¾
- RMSE: {test_rmse:.4f}
- ç›¸é—œä¿‚æ•¸ (R): {test_corr:.4f}
- æº–ç¢ºç‡ (Îµ={epsilon_value:.2f}): {final_val_accuracy:.2%}
---
å ±å‘Šç”Ÿæˆæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        st.download_button("ä¸‹è¼‰å®Œæ•´å ±å‘Š (TXT)", report_content.encode('utf-8'), f"{selected_station_name}_{selected_param_col}_GRU_report.txt", "text/plain", use_container_width=True, help="ä¸‹è¼‰åŒ…å«æ‰€æœ‰è¨­å®šèˆ‡çµæœçš„æ–‡æœ¬å ±å‘Š")
