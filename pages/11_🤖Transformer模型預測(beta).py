import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import os
import json
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention
from keras.callbacks import EarlyStopping
import glob

from utils.helpers import get_station_name_from_id, initialize_session_state, load_data 

# è¨­ç½® TensorFlow æ—¥èªŒç´šåˆ¥ï¼ŒæŠ‘åˆ¶ INFO è¨Šæ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 

# --- Streamlit é é¢è¨­å®š (å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="Transformer æ¨¡å‹é æ¸¬",
    page_icon="ğŸ¤–",
    layout="wide"
)
initialize_session_state()

st.title("ğŸ¤– æµ·æ´‹æ•¸æ“š Transformer æ¨¡å‹é æ¸¬")
st.markdown("ä½¿ç”¨ Transformer é¡ç¥ç¶“ç¶²çµ¡é æ¸¬æµ·æ´‹æ•¸æ“šçš„æœªä¾†è¶¨å‹¢ã€‚")

def analyze_data_quality(df, relevant_params):
    """åˆ†ææ•¸æ“šå“è³ªï¼Œæä¾›ç¼ºå¤±å€¼ã€é›¶å€¼ã€è² å€¼å’Œç•°å¸¸å€¼å ±å‘Šã€‚"""
    quality_metrics = {}
    for param_col in relevant_params:
        if param_col in df.columns:
            total_records = len(df)
            missing_count = df[param_col].isnull().sum()
            valid_count = total_records - missing_count
            missing_percentage = (missing_count / total_records) * 100 if total_records > 0 else 0

            if pd.api.types.is_numeric_dtype(df[param_col]):
                zero_count = (df[param_col] == 0).sum()
                negative_count = (df[param_col] < 0).sum()
                outlier_iqr_count = 0

                if valid_count > 0:
                    Q1 = df[param_col].quantile(0.25)
                    Q3 = df[param_col].quantile(0.75)
                    IQR = Q3 - Q1
                    if IQR > 0:
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        outlier_iqr_count = df[(df[param_col] < lower_bound) | (df[param_col] > upper_bound)][param_col].count()

                quality_metrics[param_col] = {
                    'total_records': total_records,
                    'valid_count': valid_count,
                    'missing_count': missing_count,
                    'missing_percentage': missing_percentage,
                    'zero_count': zero_count,
                    'negative_count': negative_count,
                    'outlier_iqr_count': outlier_iqr_count
                }
            else:
                quality_metrics[param_col] = {
                    'total_records': total_records, 'valid_count': valid_count, 'missing_count': missing_count,
                    'missing_percentage': 100.0, 'is_numeric': False
                }
        else:
            quality_metrics[param_col] = {
                'total_records': 0, 'valid_count': 0, 'missing_count': 0,
                'missing_percentage': 100.0, 'is_numeric': False, 'status': 'åˆ—ä¸å­˜åœ¨'
            }
    return quality_metrics


# --- Transformer æ¨¡å‹è¼”åŠ©å‡½æ•¸ ---
def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0.2):
    """
    æ§‹å»ºä¸€å€‹åŸºæ–¼ Transformer ç·¨ç¢¼å™¨çµæ§‹çš„æ™‚é–“åºåˆ—é æ¸¬æ¨¡å‹ã€‚
    input_shape: (sequence_length, features)
    """
    inputs = Input(shape=input_shape)
    x = inputs

    # Transformer Blocks
    for _ in range(num_transformer_blocks):
        # Attention and Normalization
        x = LayerNormalization(epsilon=1e-6)(x)
        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)
        x = x + attn_output # Residual connection
        x = LayerNormalization(epsilon=1e-6)(x)

        # Feed Forward Network
        ffn_output = Dense(ff_dim, activation="relu")(x)
        ffn_output = Dropout(dropout)(ffn_output)
        ffn_output = Dense(input_shape[-1])(ffn_output) # Output dim matches input feature dim
        x = x + ffn_output # Residual connection

    # Flatten the output for the MLP head
    x = tf.keras.layers.GlobalAveragePooling1D()(x) # åŒ¯ç¸½åºåˆ—ä¿¡æ¯ï¼Œå°‡ (batch_size, sequence_length, features) è®Šæˆ (batch_size, features)
    
    # MLP Head
    for dim in mlp_units:
        x = Dense(dim, activation="relu")(x)
        x = Dropout(dropout)(x)
    
    outputs = Dense(1)(x) # è¼¸å‡ºå–®å€‹é æ¸¬å€¼

    return Model(inputs=inputs, outputs=outputs)

def create_sequences(data, look_back):
    """
    å°‡æ™‚é–“åºåˆ—æ•¸æ“šè½‰æ›ç‚ºæ¨¡å‹æ‰€éœ€çš„åºåˆ— (X) å’Œç›®æ¨™ (y)ã€‚
    X: éå» look_back å€‹æ™‚é–“æ­¥çš„æ•¸æ“š
    y: ä¸‹ä¸€å€‹æ™‚é–“æ­¥çš„æ•¸æ“š
    """
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i + look_back), 0]) # ç²å–æ­·å²åºåˆ—
        y.append(data[i + look_back, 0])     # ç²å–ç›®æ¨™å€¼
    return np.array(X), np.array(y)


# --- å´é‚Šæ¬„ï¼šTransformer é æ¸¬è¨­å®šæ§åˆ¶é … ---
st.sidebar.header("Transformer é æ¸¬è¨­å®š")

locations = st.session_state.get('locations', [])
if not locations:
    st.sidebar.warning("è«‹åœ¨ `config.json` çš„ `STATION_COORDS` ä¸­é…ç½®æ¸¬ç«™è³‡è¨Šã€‚")
    st.stop()

selected_station = st.sidebar.selectbox("é¸æ“‡æ¸¬ç«™:", locations, key='pages_11_transformer_station', format_func=get_station_name_from_id)
selected_station_name = get_station_name_from_id(selected_station)

predictable_params_config_map = {
    col_name: info["display_zh"] for col_name, info in st.session_state.parameter_info.items()
    if info.get("type") == "linear" # åªé¸æ“‡ç·šæ€§åƒæ•¸é€²è¡Œé æ¸¬
}

# é è¼‰å…¥æ•¸æ“šä»¥å‹•æ…‹ç²å–å¯ç”¨åƒæ•¸
df_initial_check = load_data(selected_station, st.session_state.parameter_info)

available_predictable_params_display_to_col = {}
for col_name, display_name in predictable_params_config_map.items():
    col_name = col_name.lower()
    if col_name in df_initial_check.columns and pd.api.types.is_numeric_dtype(df_initial_check[col_name]):
        if df_initial_check[col_name].count() > 0: # ç¢ºä¿è©²åˆ—æœ‰éç©ºæ•¸æ“š
            available_predictable_params_display_to_col[display_name] = col_name

if not available_predictable_params_display_to_col:
    st.sidebar.error("è¼‰å…¥æ•¸æ“šå¾Œï¼Œæ²’æœ‰å¯ä¾›é æ¸¬çš„æœ‰æ•ˆæ•¸å€¼å‹åƒæ•¸ã€‚è«‹æª¢æŸ¥æ•¸æ“šæ–‡ä»¶å’Œ `config.json` ä¸­çš„åƒæ•¸é…ç½®ã€‚")
    st.stop()

selected_param_display = st.sidebar.selectbox("é¸æ“‡é æ¸¬åƒæ•¸:", list(available_predictable_params_display_to_col.keys()), key='pages_11_transformer_param_display')
selected_param_col = available_predictable_params_display_to_col[selected_param_display]

# Find value by "column_name_in_data"
param_info_original = next((value for value in st.session_state.parameter_info.values() if value.get("column_name_in_data") == selected_param_col), {})
selected_param_display_original = param_info_original.get("display_zh", selected_param_col)
param_unit = param_info_original.get("unit", "")


st.sidebar.markdown("---")
st.sidebar.subheader("é æ¸¬æ™‚é–“è¨­å®š")

prediction_frequencies = {
    "å°æ™‚ (H)": "h",
    "å¤© (D)": "D",
    "é€± (W)": "W",
    "æœˆ (M)": "M",
    "å¹´ (Y)": "Y"
}
selected_prediction_freq_display = st.sidebar.selectbox(
    "é¸æ“‡é æ¸¬é »æ¬¡:",
    list(prediction_frequencies.keys()),
    key='pages_11_prediction_frequency'
)
selected_freq_pandas = prediction_frequencies[selected_prediction_freq_display]

forecast_period_value = st.sidebar.number_input(
    f"é æ¸¬æœªä¾†å¤šä¹… ({selected_prediction_freq_display.split(' ')[0]}):",
    min_value=1,
    max_value=365 if selected_freq_pandas == 'D' else 8760 if selected_freq_pandas == 'h' else 12, # èª¿æ•´æœ€å¤§å€¼
    value=24 if selected_freq_pandas == 'h' else 7 if selected_freq_pandas == 'D' else 1,
    step=1,
    key='pages_11_forecast_period_value'
)

# --- æ•¸æ“šè¨“ç·´æ™‚é–“ç¯„åœé¸æ“‡ ---
st.sidebar.markdown("---")
st.sidebar.subheader("è¨“ç·´æ•¸æ“šæ™‚é–“ç¯„åœ")

if not df_initial_check.empty and 'ds' in df_initial_check.columns:
    min_date_available = df_initial_check['ds'].min().date()
    max_date_available = df_initial_check['ds'].max().date()
else:
    min_date_available = pd.to_datetime('1990-01-01').date() # é è¨­èµ·å§‹æ—¥æœŸ
    max_date_available = pd.Timestamp.now().date() # é è¨­çµæŸæ—¥æœŸ
    st.warning("ç„¡æ³•å¾è¼‰å…¥çš„æ•¸æ“šä¸­ç²å–æ™‚é–“ç¯„åœã€‚ä½¿ç”¨é è¨­æ—¥æœŸç¯„åœã€‚")

default_start_date = min_date_available
default_end_date = max_date_available

train_start_date = st.sidebar.date_input(
    "è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸ:",
    value=default_start_date,
    min_value=min_date_available,
    max_value=max_date_available,
    key='pages_11_train_start_date'
)
train_end_date = st.sidebar.date_input(
    "è¨“ç·´æ•¸æ“šçµæŸæ—¥æœŸ:",
    value=default_end_date,
    min_value=min_date_available,
    max_value=max_date_available,
    key='pages_11_train_end_date'
)

if train_start_date >= train_end_date:
    st.sidebar.error("è¨“ç·´æ•¸æ“šé–‹å§‹æ—¥æœŸå¿…é ˆæ—©æ–¼çµæŸæ—¥æœŸã€‚")
    st.stop()


# --- æ•¸æ“šé è™•ç†é¸é … ---
st.sidebar.markdown("---")
st.sidebar.subheader("æ•¸æ“šé è™•ç†")
missing_value_strategy = st.sidebar.selectbox(
    "ç¼ºå¤±å€¼è™•ç†:",
    options=['å‰å‘å¡«å…… (ffill)', 'å¾Œå‘å¡«å…… (bfill)', 'ç·šæ€§æ’å€¼ (interpolate)', 'ç§»é™¤ç¼ºå¤±å€¼ (dropna)'],
    key='pages_11_missing_strategy'
)

apply_smoothing = st.sidebar.checkbox("æ‡‰ç”¨æ•¸æ“šå¹³æ»‘", value=False, key='pages_11_apply_smoothing')
smoothing_window = 1

if apply_smoothing:
    smoothing_window = st.sidebar.slider("å¹³æ»‘è™•ç† (ç§»å‹•å¹³å‡è¦–çª—):", min_value=1, max_value=24, value=3, step=1,
                                         help="ç§»å‹•å¹³å‡è¦–çª—å¤§å°ï¼ˆå–®ä½èˆ‡é æ¸¬é »æ¬¡ç›¸åŒï¼‰ã€‚1 è¡¨ç¤ºä¸é€²è¡Œå¹³æ»‘è™•ç†ã€‚æ•¸å€¼è¶Šå¤§ï¼Œæ•¸æ“šè¶Šå¹³æ»‘ï¼Œä½†å¯èƒ½ä¸Ÿå¤±ç´°ç¯€ã€‚")


# --- Transformer æ¨¡å‹åƒæ•¸ ---
st.sidebar.markdown("---")
st.sidebar.subheader("Transformer æ¨¡å‹åƒæ•¸")

look_back = st.sidebar.slider("å›æº¯æ™‚é–“æ­¥ (look_back):", min_value=1, max_value=48, value=12, step=1,
                              help="Transformer æ¨¡å‹åœ¨é æ¸¬ä¸‹ä¸€å€‹æ™‚é–“é»æ™‚è€ƒæ…®å¤šå°‘å€‹éå»çš„æ™‚é–“é»ã€‚")
head_size = st.sidebar.slider("æ³¨æ„åŠ›é ­ç¶­åº¦ (head_size):", min_value=32, max_value=256, value=64, step=32,
                              help="Transformer æ³¨æ„åŠ›é ­çš„ç¶­åº¦ã€‚å»ºè­°ç‚º 2 çš„å†ªæ¬¡ã€‚")
num_heads = st.sidebar.slider("æ³¨æ„åŠ›é ­æ•¸é‡ (num_heads):", min_value=1, max_value=8, value=4, step=1,
                             help="å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶ä¸­çš„é ­æ•¸é‡ã€‚ç¢ºä¿ head_size èƒ½è¢« num_heads æ•´é™¤ã€‚")
ff_dim = st.sidebar.slider("å‰é¥‹ç¶²çµ¡ç¶­åº¦ (ff_dim):", min_value=64, max_value=512, value=128, step=64,
                           help="Transformer å¡Šä¸­å‰é¥‹ç¶²çµ¡å±¤çš„ç¶­åº¦ã€‚é€šå¸¸ç‚º head_size çš„ 2-4 å€ã€‚")
num_transformer_blocks = st.sidebar.slider("Transformer å€å¡Šæ•¸é‡:", min_value=1, max_value=5, value=2, step=1,
                                            help="æ¨¡å‹ä¸­å †ç–Šçš„ Transformer ç·¨ç¢¼å™¨å€å¡Šçš„æ•¸é‡ã€‚")
mlp_units_options = [32, 64, 128, 256]
mlp_units = st.sidebar.multiselect("MLP å±¤å–®å…ƒæ•¸:", options=mlp_units_options, default=[128],
                                   help="é æ¸¬é ­éƒ¨çš„å¤šå±¤æ„ŸçŸ¥å™¨ (MLP) å±¤çš„å–®å…ƒæ•¸ã€‚å¯ä»¥æœ‰å¤šå±¤ã€‚")
epochs = st.sidebar.number_input("è¨“ç·´è¿­ä»£æ¬¡æ•¸ (Epochs):", min_value=10, max_value=500, value=100, step=10,
                                 help="æ¨¡å‹åœ¨æ•´å€‹è¨“ç·´æ•¸æ“šé›†ä¸Šé€²è¡Œè¨“ç·´çš„æ¬¡æ•¸ã€‚")
batch_size = st.sidebar.number_input("æ‰¹æ¬¡å¤§å° (Batch Size):", min_value=1, max_value=128, value=32, step=8,
                                     help="æ¯æ¬¡è¨“ç·´è¿­ä»£ä¸­ä½¿ç”¨çš„æ¨£æœ¬æ•¸é‡ã€‚")
dropout_rate = st.sidebar.slider("Dropout æ¯”ç‡:", min_value=0.0, max_value=0.5, value=0.2, step=0.05,
                                 help="é˜²æ­¢éæ“¬åˆçš„ Dropout å±¤æ¯”ç‡ã€‚")
validation_split = st.sidebar.slider("é©—è­‰é›†æ¯”ä¾‹:", min_value=0.0, max_value=0.5, value=0.1, step=0.05,
                                     help="ç”¨æ–¼æ¨¡å‹è¨“ç·´æœŸé–“é©—è­‰çš„æ•¸æ“šæ¯”ä¾‹ã€‚")
patience = st.sidebar.number_input("æ—©åœè€å¿ƒå€¼ (Patience):", min_value=5, max_value=50, value=10, step=5,
                                  help="å¦‚æœé©—è­‰æå¤±åœ¨é€™éº¼å¤šå€‹ epochs å…§æ²’æœ‰æ”¹å–„ï¼Œè¨“ç·´å°‡åœæ­¢ã€‚")

# --- åŸ·è¡Œé æ¸¬æŒ‰éˆ• ---
if st.sidebar.button("ğŸ¤– åŸ·è¡Œ Transformer é æ¸¬"):
    # æª¢æŸ¥ TensorFlow æ˜¯å¦å¯ç”¨
    if not tf.test.is_built_with_cuda() and not tf.config.list_physical_devices('GPU'):
        st.warning("è­¦å‘Š: TensorFlow æœªå•Ÿç”¨ GPU åŠ é€Ÿã€‚æ¨¡å‹è¨“ç·´å¯èƒ½è¼ƒæ…¢ã€‚")

    df = df_initial_check


    if df.empty or selected_param_col not in df.columns:
        if df.empty:
            st.error(f"æ‰€é¸æ¸¬ç«™ '{selected_station_name}' æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ•¸æ“šã€‚")
        else:
            st.error(f"æ‰€é¸æ¸¬ç«™ '{selected_station_name}' çš„æ•¸æ“šæ–‡ä»¶ç¼ºå°‘åƒæ•¸ '{selected_param_display_original}' (åŸå§‹åˆ—å: '{selected_param_col}')ã€‚")
            st.info(f"æ•¸æ“šä¸­å¯ç”¨çš„åˆ—: {df.columns.tolist()}")
        st.stop()

    st.info(f"æ­£åœ¨å°æ¸¬ç«™ **{selected_station_name}** çš„åƒæ•¸ **{selected_param_display_original}** åŸ·è¡Œ Transformer é æ¸¬...")

    # --- æ•¸æ“šé è™•ç† ---
    df_processed = df[['ds', selected_param_col]].copy()
    df_processed.columns = ['ds', 'y']

    df_processed['ds'] = pd.to_datetime(df_processed['ds'])
    df_processed.sort_values('ds', inplace=True)

    # æ ¹æ“šé¸å®šçš„è¨“ç·´æ—¥æœŸç¯„åœç¯©é¸æ•¸æ“š
    train_start_datetime = pd.to_datetime(train_start_date)
    # çµæŸæ—¥æœŸåŒ…å«ç•¶å¤©æ‰€æœ‰æ™‚é–“
    train_end_datetime = pd.to_datetime(train_end_date) + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1) 

    df_processed = df_processed[
        (df_processed['ds'] >= train_start_datetime) &
        (df_processed['ds'] <= train_end_datetime)
    ].copy()

    if df_processed.empty:
        st.error(f"åœ¨é¸å®šçš„è¨“ç·´æ™‚é–“ç¯„åœ ({train_start_date} è‡³ {train_end_date}) å…§æ²’æœ‰æ‰¾åˆ°æ•¸æ“šã€‚è«‹èª¿æ•´æ™‚é–“ç¯„åœã€‚")
        st.stop()
    
    # ç§»é™¤é‡è¤‡çš„æ™‚é–“æˆ³ (åœ¨é‡æ¡æ¨£å‰è™•ç†)
    if df_processed['ds'].duplicated().any():
        st.warning("è­¦å‘Šï¼šè¨“ç·´æ•¸æ“šä¸­å­˜åœ¨é‡è¤‡çš„æ™‚é–“æˆ³ï¼Œå°‡ç§»é™¤é‡è¤‡é …ã€‚")
        df_processed.drop_duplicates(subset=['ds'], keep='first', inplace=True)

    # é‡æ¡æ¨£ä¸¦è¨ˆç®—å¹³å‡å€¼
    df_processed = df_processed.set_index('ds').resample(selected_freq_pandas).mean().reset_index()

    # ç¼ºå¤±å€¼è™•ç†
    if missing_value_strategy == 'å‰å‘å¡«å…… (ffill)':
        df_processed['y'] = df_processed['y'].ffill()
    elif missing_value_strategy == 'å¾Œå‘å¡«å…… (bfill)':
        df_processed['y'] = df_processed['y'].bfill()
    elif missing_value_strategy == 'ç·šæ€§æ’å€¼ (interpolate)':
        df_processed['y'] = df_processed['y'].interpolate(method='linear')
    elif missing_value_strategy == 'ç§»é™¤ç¼ºå¤±å€¼ (dropna)':
        df_processed = df_processed.dropna(subset=['y'])

    # æª¢æŸ¥è™•ç†å¾Œæ˜¯å¦ä»æœ‰æœ‰æ•ˆæ•¸æ“š
    if df_processed['y'].isnull().all():
        st.error(f"åœ¨ç¶“éé è™•ç†å¾Œï¼Œåƒæ•¸ '{selected_param_display}' çš„æ•¸æ“šå…¨éƒ¨ç‚ºç¼ºå¤±å€¼ã€‚ç„¡æ³•é€²è¡Œé æ¸¬ã€‚")
        st.stop()

    # æ‡‰ç”¨å¹³æ»‘
    if apply_smoothing and smoothing_window > 1:
        df_processed['y'] = df_processed['y'].rolling(window=smoothing_window, min_periods=1, center=True).mean()

    # å†æ¬¡ç§»é™¤ä»»ä½•å‰©é¤˜çš„ NaN (å¯èƒ½ä¾†è‡ªå¹³æ»‘æˆ–æ’å€¼é‚Šç·£)
    df_processed.dropna(subset=['ds', 'y'], inplace=True)

    if df_processed.empty:
        st.error("ç¶“éæ•¸æ“šé è™•ç†å’Œæ™‚é–“ç¯„åœç¯©é¸å¾Œï¼Œæ²’æœ‰è¶³å¤ çš„æœ‰æ•ˆæ•¸æ“šç”¨æ–¼é æ¸¬ã€‚è«‹æª¢æŸ¥åŸå§‹æ•¸æ“šã€æ™‚é–“ç¯„åœå’Œé è™•ç†é¸é …ã€‚")
        st.stop()
    
    # æª¢æŸ¥æ•¸æ“šé•·åº¦æ˜¯å¦æ»¿è¶³ look_back è¦æ±‚
    if len(df_processed) <= look_back:
        st.error(f"è¨“ç·´æ•¸æ“šé•·åº¦ ({len(df_processed)}) å¿…é ˆå¤§æ–¼å›æº¯æ™‚é–“æ­¥é•· ({look_back})ã€‚è«‹å¢åŠ æ•¸æ“šç¯„åœæˆ–æ¸›å°‘å›æº¯æ™‚é–“æ­¥é•·ã€‚")
        st.stop()

    # æ•¸æ“šæ­¸ä¸€åŒ– (0-1 ç¯„åœ)
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(df_processed['y'].values.reshape(-1, 1))

    # å‰µå»º Transformer åºåˆ— (X, y)
    X, y = create_sequences(scaled_data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1)) # ç‚º Transformer å¢åŠ ç‰¹å¾µç¶­åº¦ (batch, sequence_length, features)

    # è¨“ç·´é›†å’Œæ¸¬è©¦é›†åŠƒåˆ† (æ‰‹å‹•åŠƒåˆ†)
    train_size = int(len(X) * (1 - validation_split))
    X_train, X_test = X[0:train_size,:], X[train_size:len(X),:]
    y_train, y_test = y[0:train_size], y[train_size:len(y)]

    # --- å»ºç«‹ä¸¦è¨“ç·´ Transformer æ¨¡å‹ ---
    with st.spinner("æ­£åœ¨å»ºç«‹ä¸¦è¨“ç·´ Transformer æ¨¡å‹..."):
        try:
            model = build_transformer_model(
                input_shape=(look_back, 1), # (sequence_length, features)
                head_size=head_size,
                num_heads=num_heads,
                ff_dim=ff_dim,
                num_transformer_blocks=num_transformer_blocks,
                mlp_units=mlp_units,
                dropout=dropout_rate
            )
            model.compile(optimizer='adam', loss='mean_squared_error')

            # å®šç¾©æ—©åœå›èª¿ï¼Œé˜²æ­¢éæ“¬åˆ
            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

            # è¨“ç·´æ¨¡å‹
            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                                validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)
        except Exception as e:
            st.error(f"Transformer æ¨¡å‹è¨“ç·´å¤±æ•—ï¼š{e}ã€‚è«‹æª¢æŸ¥æ•¸æ“šæˆ–èª¿æ•´æ¨¡å‹åƒæ•¸ã€‚")
            st.stop()

    st.success("Transformer æ¨¡å‹è¨“ç·´å®Œæˆï¼")

    ### ğŸ“š è¨“ç·´æ•¸æ“šæ¦‚è¦½
    st.subheader("ğŸ“Š è¨“ç·´æ•¸æ“šæ¦‚è¦½")
    if not df_processed.empty:
        total_duration = df_processed['ds'].max() - df_processed['ds'].min()
        total_records = len(df_processed)
        inferred_freq = None
        try:
            inferred_freq = pd.infer_freq(df_processed['ds'])
        except ValueError:
            inferred_freq = 'ç„¡æ³•ç²¾ç¢ºæ¨æ–· (æ•¸æ“šå¯èƒ½é–“éš”ä¸ä¸€è‡´)'
        st.write(f"**ä½¿ç”¨æ•¸æ“šå€é–“**: å¾ **{df_processed['ds'].min().strftime('%Y-%m-%d %H:%M')}** åˆ° **{df_processed['ds'].max().strftime('%Y-%m-%d %H:%M')}**")
        st.write(f"**ç¸½æ™‚é•·**: **{total_duration}**")
        st.write(f"**ç¸½ç­†æ•¸**: **{total_records}** ç­†")
        st.write(f"**æ•¸æ“šé »æ¬¡ (é è™•ç†å¾Œ)**: **{selected_freq_pandas}** (åŸå§‹æ¨æ–·: **{inferred_freq}**)")
    else:
        st.warning("æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ•¸æ“šæ¦‚è¦½ã€‚")

    ### ğŸ“Š æ•¸æ“šå“è³ªæ¦‚è¦½
    st.subheader("ğŸ” è¨“ç·´æ•¸æ“šå“è³ªå ±å‘Š")
    df_for_quality_check = df_processed.set_index('ds').rename(columns={'y': selected_param_col}).copy()
    quality_report = analyze_data_quality(df_for_quality_check, relevant_params=[selected_param_col])

    if selected_param_col in quality_report:
        metrics = quality_report[selected_param_col]
        st.write(f"**åƒæ•¸: {selected_param_display_original}**")
        st.write(f"- ç¸½è¨˜éŒ„æ•¸: {metrics['total_records']}")
        st.write(f"- æœ‰æ•ˆè¨˜éŒ„æ•¸: {metrics['valid_count']}")
        st.write(f"- ç¼ºå¤±å€¼æ•¸é‡: {metrics['missing_count']} (**{metrics['missing_percentage']:.2f}%**)")
        if metrics.get('is_numeric', True):
            st.write(f"- é›¶å€¼æ•¸é‡: {metrics['zero_count']}")
            st.write(f"- è² å€¼æ•¸é‡: {metrics['negative_count']}")
            st.write(f"- æ½›åœ¨ IQR ç•°å¸¸å€¼æ•¸é‡: {metrics['outlier_iqr_count']}")

            quality_data = {
                'é¡å‹': ['æœ‰æ•ˆå€¼', 'ç¼ºå¤±å€¼', 'é›¶å€¼', 'è² å€¼', 'æ½›åœ¨ç•°å¸¸å€¼'],
                'æ•¸é‡': [
                    metrics['valid_count'],
                    metrics['missing_count'],
                    metrics['zero_count'],
                    metrics['negative_count'],
                    metrics['outlier_iqr_count']
                ]
            }
            quality_df = pd.DataFrame(quality_data)
            quality_df = quality_df[quality_df['æ•¸é‡'] > 0] # åªé¡¯ç¤ºæ•¸é‡å¤§æ–¼ 0 çš„é¡åˆ¥

            if not quality_df.empty:
                fig_quality = px.pie(
                    quality_df,
                    values='æ•¸é‡',
                    names='é¡å‹',
                    title=f"'{selected_param_display_original}' æ•¸æ“šå“è³ªåˆ†ä½ˆ",
                    hole=0.3,
                    color_discrete_sequence=px.colors.qualitative.Pastel
                )
                fig_quality.update_traces(textposition='inside', textinfo='percent+label', marker=dict(line=dict(color='#000000', width=1)))
                
                fig_quality.update_layout(showlegend=True)
                
                st.plotly_chart(fig_quality, use_container_width=True)
            else:
                st.info("æ•¸æ“šå“è³ªéå¸¸é«˜ï¼Œæ²’æœ‰ç¼ºå¤±å€¼ã€é›¶å€¼ã€è² å€¼æˆ–ç•°å¸¸å€¼ã€‚")
    else:
        st.warning(f"ç„¡æ³•ç‚ºåƒæ•¸ '{selected_param_display_original}' ç”Ÿæˆæ•¸æ“šå“è³ªå ±å‘Šã€‚")


    ### ğŸ¯ æ¨¡å‹è©•ä¼° (æ­·å²æ•¸æ“š)
    st.subheader("ğŸ“‰ æ¨¡å‹æ€§èƒ½è©•ä¼°")

    # è¨“ç·´é›†é æ¸¬
    train_predict = model.predict(X_train)
    train_predict = scaler.inverse_transform(train_predict) # åæ­¸ä¸€åŒ–
    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))

    # æ¸¬è©¦é›†é æ¸¬
    test_predict = model.predict(X_test)
    test_predict = scaler.inverse_transform(test_predict) # åæ­¸ä¸€åŒ–
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

    # è¨ˆç®— RMSE (å‡æ–¹æ ¹èª¤å·®)
    train_rmse = np.sqrt(mean_squared_error(y_train_actual, train_predict))
    test_rmse = np.sqrt(mean_squared_error(y_test_actual, test_predict))

    st.metric(label=f"è¨“ç·´é›† RMSE for {selected_param_display_original}", value=f"{train_rmse:.4f}")
    st.metric(label=f"æ¸¬è©¦é›† RMSE for {selected_param_display_original}", value=f"{test_rmse:.4f}")
    st.info("**RMSE (å‡æ–¹æ ¹èª¤å·®)** è¡¡é‡æ¨¡å‹åœ¨æ­·å²æ•¸æ“šä¸Šçš„é æ¸¬èª¤å·®ï¼Œå€¼è¶Šå°è¡¨ç¤ºæ¨¡å‹è¶Šç²¾ç¢ºã€‚**æ¸¬è©¦é›† RMSE** åæ˜ æ¨¡å‹å°æœªè¦‹æ•¸æ“šçš„æ³›åŒ–èƒ½åŠ›ã€‚")

    # è¨“ç·´éç¨‹ä¸­çš„æå¤±æ›²ç·š
    st.subheader("ğŸ“ˆ æ¨¡å‹è¨“ç·´æå¤±æ›²ç·š")
    fig_loss = go.Figure()
    fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='è¨“ç·´æå¤±'))
    if 'val_loss' in history.history: # å¦‚æœæœ‰é©—è­‰é›†ï¼Œå‰‡é¡¯ç¤ºé©—è­‰æå¤±
        fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='é©—è­‰æå¤±'))

    fig_loss.update_layout(
        title="è¨“ç·´èˆ‡é©—è­‰æå¤±",
        xaxis_title="Epoch",
        yaxis_title="æå¤± (MSE)",
        height=400
    )
    st.plotly_chart(fig_loss, use_container_width=True)


    ### ğŸ“Š é æ¸¬çµæœè¦–è¦ºåŒ–
    st.subheader("æœªä¾†è¶¨å‹¢é æ¸¬")

    # æœªä¾†é æ¸¬ï¼šå¾æœ€å¾Œ `look_back` å€‹é»é–‹å§‹ï¼Œè¿­ä»£é æ¸¬æœªä¾†å€¼
    # ç¢ºä¿ last_sequence æ˜¯ NumPy arrayï¼Œå½¢ç‹€ç‚º (look_back, 1)
    last_sequence = scaled_data[-look_back:] 
    future_predictions = []

    with st.spinner(f"æ­£åœ¨é æ¸¬æœªä¾† {forecast_period_value} å€‹æ™‚é–“é»..."):
        for _ in range(forecast_period_value):
            # é æ¸¬ä¸‹ä¸€å€‹æ™‚é–“é»ï¼Œæ¨¡å‹è¼¸å…¥éœ€è¦æ˜¯ (1, look_back, 1)
            next_pred = model.predict(last_sequence.reshape(1, look_back, 1), verbose=0)[0, 0] # verbose=0 æŠ‘åˆ¶é æ¸¬æ™‚çš„é€²åº¦æ¢
            future_predictions.append(next_pred)
            # æ›´æ–°åºåˆ—ï¼šç§»é™¤æœ€èˆŠçš„é»ï¼ŒåŠ å…¥æ–°çš„é æ¸¬é»
            last_sequence = np.append(last_sequence[1:], [[next_pred]], axis=0)

    # åæ­¸ä¸€åŒ–æœªä¾†é æ¸¬å€¼
    future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))

    # ç”Ÿæˆæœªä¾†æ™‚é–“æˆ³
    last_known_date = df_processed['ds'].max()
    future_dates = pd.date_range(start=last_known_date + pd.to_timedelta(1, unit=selected_freq_pandas),
                                 periods=forecast_period_value,
                                 freq=selected_freq_pandas)

    forecast_df = pd.DataFrame({
        'ds': future_dates,
        'yhat': future_predictions.flatten()
    })

    # åˆä½µæ‰€æœ‰çµæœç”¨æ–¼ç¹ªåœ–
    full_plot_df = df_processed.copy()
    full_plot_df['yhat_train'] = np.nan
    full_plot_df['yhat_test'] = np.nan

    # å¡«å……æ­·å²é æ¸¬å€¼
    # è¨“ç·´é æ¸¬çš„ç´¢å¼•å¾ look_back é–‹å§‹
    full_plot_df.loc[df_processed.index[look_back : look_back + len(train_predict)], 'yhat_train'] = train_predict.flatten()
    # æ¸¬è©¦é æ¸¬çš„ç´¢å¼•å¾è¨“ç·´é æ¸¬ä¹‹å¾Œé–‹å§‹
    full_plot_df.loc[df_processed.index[look_back + len(train_predict) : look_back + len(train_predict) + len(test_predict)], 'yhat_test'] = test_predict.flatten()

    fig = go.Figure()

    # å¯¦éš›æ•¸æ“š (è—è‰²å¯¦ç·š)
    fig.add_trace(go.Scatter(
        x=full_plot_df['ds'],
        y=full_plot_df['y'],
        mode='lines',
        name='å¯¦éš›æ•¸æ“š',
        line=dict(color='blue')
    ))

    # è¨“ç·´é›†é æ¸¬ (ç¶ è‰²è™›ç·š)
    fig.add_trace(go.Scatter(
        x=full_plot_df['ds'],
        y=full_plot_df['yhat_train'],
        mode='lines',
        name='è¨“ç·´é›†é æ¸¬',
        line=dict(color='green', dash='dot')
    ))

    # æ¸¬è©¦é›†é æ¸¬ (æ©™è‰²è™›ç·š)
    fig.add_trace(go.Scatter(
        x=full_plot_df['ds'],
        y=full_plot_df['yhat_test'],
        mode='lines',
        name='æ¸¬è©¦é›†é æ¸¬',
        line=dict(color='orange', dash='dot')
    ))

    # æœªä¾†é æ¸¬ (ç´…è‰²è™›ç·š)
    fig.add_trace(go.Scatter(
        x=forecast_df['ds'],
        y=forecast_df['yhat'],
        mode='lines',
        name='æœªä¾†é æ¸¬',
        line=dict(color='red', dash='dash', width=2)
    ))

    forecast_unit_display = selected_prediction_freq_display.split(' ')[0]
    
    # æ‡‰ç”¨ä¸­æ–‡å­—é«” (å¦‚æœé…ç½®äº†)
    fig.update_layout(
        title=f"{selected_station_name} - {selected_param_display_original} Transformer æœªä¾† {forecast_period_value} {forecast_unit_display} é æ¸¬",
        xaxis_title="æ™‚é–“",
        yaxis_title=f"{selected_param_display_original} {param_unit}",
        hovermode="x unified",
        height=600
    )
    st.plotly_chart(fig, use_container_width=True)

    ### ğŸ’¾ ä¸‹è¼‰é æ¸¬çµæœ
    st.markdown("æ‚¨å¯ä»¥ä¸‹è¼‰åŒ…å«æœªä¾†é æ¸¬å€¼çš„ CSV æ–‡ä»¶ã€‚")

    download_df = forecast_df.copy()
    download_df.rename(columns={'ds': 'æ™‚é–“', 'yhat': f'é æ¸¬å€¼_{selected_param_display_original}'}, inplace=True)
    download_df['æ™‚é–“'] = download_df['æ™‚é–“'].dt.strftime('%Y-%m-%d %H:%M:%S')

    csv_data = download_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ä¸‹è¼‰ Transformer é æ¸¬ CSV æ–‡ä»¶",
        data=csv_data,
        file_name=f"{selected_station_name}_{selected_param_col}_Transformer_forecast.csv",
        mime="text/csv",
    )
